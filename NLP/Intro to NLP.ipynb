{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro to NLP.ipynb","provenance":[],"authorship_tag":"ABX9TyNFOQzlzt5YObqGw5qz56bv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"vtAgo5zYCClj"},"source":["# Natural Language Processing with TensorFlow\n","\n","The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.\n","\n","Natural language is a broad term but you can consider it to cover any of the following:\n","* Text (such as that contained in an email, blog post, book, Tweet)\n","* Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)\n","\n","Under the umbrellas of text and speech there are many different things you might want to do.\n","\n","If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).\n","\n","If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for.\n","\n","> 🔑 **Note:** Both of these types of data are often referred to as *sequences* (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called *seq2seq*, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).\n","\n","To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:\n","\n","```\n","Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n","```\n","\n","> 📖 **Resource:** For a great overview of NLP and the different problems within it, read the article [*A Simple Introduction to Natural Language Processing*](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32).\n","\n","## What we're going to cover\n","\n","Let's get specific hey?\n","\n","* Downloading a text dataset\n","* Visualizing text data\n","* Converting text into numbers using tokenization\n","* Turning our tokenized text into an embedding\n","* Modelling a text dataset\n","  * Starting with a baseline (TF-IDF)\n","  * Building several deep learning text models\n","    * Dense, LSTM, GRU, Conv1D, Transfer learning\n","* Comparing the performance of each our models\n","* Combining our models into an ensemble\n","* Saving and loading a trained model\n","* Find the most wrong predictions\n"]},{"cell_type":"code","metadata":{"id":"DEYTFigmc3CI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6570d84d-2b4e-444a-b6b7-be712056bcfa","executionInfo":{"status":"ok","timestamp":1660684380583,"user_tz":300,"elapsed":198,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check for GPU\n","!nvidia-smi -L"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-1ce0849e-680f-7537-5fc8-0ed631887d7f)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFOHPqgE8pv-","outputId":"aaa78c2d-7d64-4fe3-ec53-90cdfa82cc7e","executionInfo":{"status":"ok","timestamp":1660684401985,"user_tz":300,"elapsed":358,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Download helper functions script\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-08-16 21:13:21--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10246 (10K) [text/plain]\n","Saving to: ‘helper_functions.py’\n","\n","\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n","\n","2022-08-16 21:13:21 (98.0 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"ICFbSkoM85tq","executionInfo":{"status":"ok","timestamp":1660684406415,"user_tz":300,"elapsed":2824,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Import series of helper functions for the notebook\n","from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCZrclc2COWW"},"source":["## Download a text dataset\n","\n","Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) datset from Kaggle which contains text-based Tweets about natural disasters. \n","\n","The Real Tweets are actually about diasters, for example:\n","\n","```\n","Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n","```\n","\n","The Not Real Tweets are Tweets not about diasters (they can be on anything), for example:\n","\n","```\n","'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n","```\n","\n","For convenience, the dataset has been [downloaded from Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (doing this requires a Kaggle account) and uploaded as a downloadable zip file. \n","\n","> 🔑 **Note:** The original downloaded data has not been altered to how you would download it from Kaggle."]},{"cell_type":"code","metadata":{"id":"C0FEcci5IH8S","colab":{"base_uri":"https://localhost:8080/"},"outputId":"932acdb4-d2ef-4fc3-829d-e185f5b843a5","executionInfo":{"status":"ok","timestamp":1660684407769,"user_tz":300,"elapsed":331,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Download data (same as from Kaggle)\n","!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n","\n","# Unzip data\n","unzip_data(\"nlp_getting_started.zip\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-08-16 21:13:27--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.119.128, 108.177.121.128, 108.177.120.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.119.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 607343 (593K) [application/zip]\n","Saving to: ‘nlp_getting_started.zip’\n","\n","\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.004s  \n","\n","2022-08-16 21:13:27 (146 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"wBIR6tTI9QcR"},"source":["Unzipping `nlp_getting_started.zip` gives the following 3 `.csv` files:\n","* `sample_submission.csv` - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n","* `train.csv` - training samples of real and not real diaster Tweets.\n","* `test.csv` - testing samples of real and not real diaster Tweets."]},{"cell_type":"markdown","metadata":{"id":"7HpxZKYdD6V-"},"source":["## Visualizing a text dataset\n","\n","\n","> 📖 **Reading:** You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter `.txt` files and `.json` files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:\n","* [How to Read and Write Files in Python](https://realpython.com/read-write-files-python/)\n","* [Working with JSON Data in Python](https://realpython.com/python-json/)"]},{"cell_type":"code","metadata":{"id":"qRvkeYEJIKsw","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"9e56e590-3d91-435f-c7b7-4f02014e513b","executionInfo":{"status":"ok","timestamp":1660684415589,"user_tz":300,"elapsed":659,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Turn .csv files into pandas DataFrame's\n","import pandas as pd\n","train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","train_df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "],"text/html":["\n","  <div id=\"df-df428492-bd27-4e3b-89e7-228152538dc0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df428492-bd27-4e3b-89e7-228152538dc0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-df428492-bd27-4e3b-89e7-228152538dc0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-df428492-bd27-4e3b-89e7-228152538dc0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"1xGqlnQaLmaT"},"source":["The training data we downloaded is probably shuffled already. But just to be sure, let's shuffle it again."]},{"cell_type":"code","metadata":{"id":"ACCE7h6OMVjR","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"50cf5bdc-634e-4e93-abc0-a39b90bcd85d","executionInfo":{"status":"ok","timestamp":1660684417471,"user_tz":300,"elapsed":157,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Shuffle training dataframe\n","train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with 100% (fraction) of data with random_state=42 for reproducibility \n","train_df_shuffled.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id      keyword               location  \\\n","2644  3796  destruction                    NaN   \n","2227  3185       deluge                    NaN   \n","5448  7769       police                     UK   \n","132    191   aftershock                    NaN   \n","6845  9810       trauma  Montgomery County, MD   \n","\n","                                                   text  target  \n","2644  So you have a new weapon that can cause un-ima...       1  \n","2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n","5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n","132   Aftershock back to school kick off was great. ...       0  \n","6845  in response to trauma Children of Addicts deve...       0  "],"text/html":["\n","  <div id=\"df-1e82e363-b6ac-4e2f-ad01-dbc7cdc092ce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2644</th>\n","      <td>3796</td>\n","      <td>destruction</td>\n","      <td>NaN</td>\n","      <td>So you have a new weapon that can cause un-ima...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2227</th>\n","      <td>3185</td>\n","      <td>deluge</td>\n","      <td>NaN</td>\n","      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5448</th>\n","      <td>7769</td>\n","      <td>police</td>\n","      <td>UK</td>\n","      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>191</td>\n","      <td>aftershock</td>\n","      <td>NaN</td>\n","      <td>Aftershock back to school kick off was great. ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6845</th>\n","      <td>9810</td>\n","      <td>trauma</td>\n","      <td>Montgomery County, MD</td>\n","      <td>in response to trauma Children of Addicts deve...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e82e363-b6ac-4e2f-ad01-dbc7cdc092ce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1e82e363-b6ac-4e2f-ad01-dbc7cdc092ce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1e82e363-b6ac-4e2f-ad01-dbc7cdc092ce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Lw4mKW1yL0kI"},"source":["Notice how the training data has a `\"target\"` column.\n","\n","We're going to be writing code to find patterns (e.g. different combinations of words) in the `\"text\"` column of the training dataset to predict the value of the `\"target\"` column.\n","\n","The test dataset doesn't have a `\"target\"` column.\n","\n","```\n","Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)\n","```\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png)\n","*Example text classification inputs and outputs for the problem of classifying whether a Tweet is about a diaster or not.*"]},{"cell_type":"code","metadata":{"id":"tDh5t7thI5BM","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"d101a900-3e9f-4ff6-883c-748028c541e5","executionInfo":{"status":"ok","timestamp":1660684420583,"user_tz":300,"elapsed":140,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# The test data doesn't have a target (that's what we'd try to predict)\n","test_df.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id keyword location                                               text\n","0   0     NaN      NaN                 Just happened a terrible car crash\n","1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n","2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n","3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n","4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"],"text/html":["\n","  <div id=\"df-436b71f8-c067-45da-83b6-86993a991d5e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-436b71f8-c067-45da-83b6-86993a991d5e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-436b71f8-c067-45da-83b6-86993a991d5e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-436b71f8-c067-45da-83b6-86993a991d5e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"O4JhBRn5Mn-V"},"source":["Let's check how many examples of each target we have."]},{"cell_type":"code","metadata":{"id":"k4P5DnLhIciD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9cc7c114-25f8-4c07-8375-c53be9299401","executionInfo":{"status":"ok","timestamp":1660684428084,"user_tz":300,"elapsed":117,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# How many examples of each class?\n","train_df.target.value_counts()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    4342\n","1    3271\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"WjEDQ297Ihy4"},"source":["Since we have two target values, we're dealing with a **binary classification** problem.\n","\n","It's fairly balanced too, about 60% negative class (`target = 0`) and 40% positive class (`target = 1`).\n","\n","Where, \n","\n","* `1` = a real disaster Tweet\n","* `0` = not a real disaster Tweet\n","\n","And what about the total number of samples we have?"]},{"cell_type":"code","metadata":{"id":"jQxg7EKKIy5L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2eb86eaf-b4d7-4e0a-ebfb-e11bf99e5964","executionInfo":{"status":"ok","timestamp":1660684429761,"user_tz":300,"elapsed":150,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# How many samples total?\n","print(f\"Total training samples: {len(train_df)}\")\n","print(f\"Total test samples: {len(test_df)}\")\n","print(f\"Total samples: {len(train_df) + len(test_df)}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training samples: 7613\n","Total test samples: 3263\n","Total samples: 10876\n"]}]},{"cell_type":"code","metadata":{"id":"vH3EXknTI3bQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b27e2aa0-8446-477e-d035-a9413de02088","executionInfo":{"status":"ok","timestamp":1660684431477,"user_tz":300,"elapsed":158,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Let's visualize some random training examples\n","import random\n","random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n","for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n","  _, text, target = row\n","  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"---\\n\")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 0 (not real disaster)\n","Text:\n","@gregorysanders @USDOT &amp; the stat of high auto deaths applies to children in a vehicle. I guess they can out run lightrail better than adult\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","And you wonder why he's injured every year https://t.co/XYiwR9JETl\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","Ari's hints and snippets will be the death of me.\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","Well unfortunately for my followers stage pics came in today. Advanced apologies for the inundationÛ_ https://t.co/u8hSrtrXMm\n","\n","---\n","\n","Target: 1 (real disaster)\n","Text:\n","Memorial unveiled for Travis County deputy killed in Sept. flooding: Travis County Sheriff Greg Hamilton joinedÛ_ http://t.co/Eo2F96WXPz\n","\n","---\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"1FhRRewGPNS_"},"source":["### Split data into training and validation sets\n","\n","* Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n","\n"]},{"cell_type":"code","metadata":{"id":"7OJf31TQ-X8s","executionInfo":{"status":"ok","timestamp":1660684435146,"user_tz":300,"elapsed":155,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# Use train_test_split to split training data into training and validation sets\n","train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n","                                                                            train_df_shuffled[\"target\"].to_numpy(),\n","                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n","                                                                            random_state=42) # random state for reproducibility"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWGOTjanBaTQ","outputId":"aa7a5757-6b56-4b3b-873b-20ba3f2eddd9","executionInfo":{"status":"ok","timestamp":1660684436355,"user_tz":300,"elapsed":142,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check the lengths\n","len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6851, 6851, 762, 762)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqhvQK9wBTbw","outputId":"085bf07d-4c55-4835-9151-df572212896e","executionInfo":{"status":"ok","timestamp":1660684437365,"user_tz":300,"elapsed":144,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# View the first 10 training sentences and their labels\n","train_sentences[:10], train_labels[:10]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n","        'Imagine getting flattened by Kurt Zouma',\n","        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n","        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n","        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n","        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n","        'destroy the free fandom honestly',\n","        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n","        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n","        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n","       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"EN-houoSD-hP"},"source":["## Converting text into numbers\n","\n","In NLP, there are two main concepts for turning text into numbers:\n","* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n","  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n","  2. **Character-level tokenization**, such as converting the letters A-Z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n","  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n","* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n","  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n","  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n","*Example of **tokenization** (straight mapping from word to number) and **embedding** (richer representation of relationships between tokens).*\n","\n","> 🤔 **Question:** What level of tokenzation should I use? What embedding should should I choose?\n","\n","It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using [`tf.keras.layers.concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)). \n","\n","If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start.\n","\n","> 🔑 **Note:** Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\"."]},{"cell_type":"markdown","metadata":{"id":"8UnRcM1PELHn"},"source":["### Text vectorization (tokenization)\n","To tokenize our words, we'll use the helpful preprocessing layer [`tf.keras.layers.experimental.preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n","\n","The `TextVectorization` layer takes the following parameters:\n","* `max_tokens` - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens. \n","* `standardize` - Method for standardizing text. Default is `\"lower_and_strip_punctuation\"` which lowers text and removes all punctuation marks.\n","* `split` - How to split text, default is `\"whitespace\"` which splits on spaces.\n","* `ngrams` - How many words to contain per token split, for example, `ngrams=2` splits tokens into continuous sequences of 2.\n","* `output_mode` -  How to output tokens, can be `\"int\"` (integer mapping), `\"binary\"` (one-hot encoding), `\"count\"` or `\"tf-idf\"`. See documentation for more.\n","* `output_sequence_length` - Length of tokenized sequence to output. For example, if `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.\n","* `pad_to_max_tokens` - Defaults to `False`, if `True`, the output feature axis will be padded to `max_tokens` even if the number of unique tokens in the vocabulary is less than `max_tokens`. Only valid in certain modes, see docs for more.\n","\n","Let's see it in action."]},{"cell_type":"code","metadata":{"id":"PVcZk-LcNunF","executionInfo":{"status":"ok","timestamp":1660684445350,"user_tz":300,"elapsed":2929,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n","# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n","\n","# Use the default TextVectorization variables\n","text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n","                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n","                                    split=\"whitespace\", # how to split tokens\n","                                    ngrams=None, # create groups of n-words?\n","                                    output_mode=\"int\", # how to map tokens to numbers\n","                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n","                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0Ej5mzKGkK8"},"source":["`TextVectorization` object with the default settings but let's customize it.\n","\n","* Set values for `max_tokens` and `output_sequence_length`.\n","\n",">For `max_tokens` (the number of words in the vocabulary), multiples of 10,000 (`10,000`, `20,000`, `30,000`) or the exact number of unique words in your text (e.g. `32,179`) are common values. For our use case, we'll use `10,000`.\n","\n",">For the `output_sequence_length` we'll use the average number of tokens per Tweet in the training set. \\"]},{"cell_type":"code","metadata":{"id":"SQ3ZCINnR56H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5dc6931a-5248-432b-ac82-14f54675d93f","executionInfo":{"status":"ok","timestamp":1660684462546,"user_tz":300,"elapsed":115,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Find average number of tokens (words) in training Tweets\n","round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"AFGTRcw8Hv7R"},"source":["Now let's create another `TextVectorization` object using our custom parameters."]},{"cell_type":"code","metadata":{"id":"eYPcGwdbafmW","executionInfo":{"status":"ok","timestamp":1660684466473,"user_tz":300,"elapsed":110,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Setup text vectorization with custom variables\n","max_vocab_length = 10000 # max number of words to have in our vocabulary\n","max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n","\n","text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n","                                    output_mode=\"int\",\n","                                    output_sequence_length=max_length)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BSWycfB3H3wV"},"source":["\n","To map our `TextVectorization` instance `text_vectorizer` to our data, we can call the `adapt()` method on it whilst passing it our training text."]},{"cell_type":"code","metadata":{"id":"0083KHXPO4m2","executionInfo":{"status":"ok","timestamp":1660684473007,"user_tz":300,"elapsed":1331,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit the text vectorizer to the training text\n","text_vectorizer.adapt(train_sentences)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Syh0VB9wIHUq"},"source":["Training data mapped! Let's try our `text_vectorizer` on a custom sentence (one similar to what you might see in the training data)."]},{"cell_type":"code","metadata":{"id":"uizmdJKvO2OW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cef3aa33-e2d8-45ea-ec10-62ce00f0161f","executionInfo":{"status":"ok","timestamp":1660684474664,"user_tz":300,"elapsed":348,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Create sample sentence and tokenize it\n","sample_sentence = \"There's a flood in my street!\"\n","text_vectorizer([sample_sentence])"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n","array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0]])>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"M0RmAeplIW57"},"source":[">Notice the 0's at the end of the returned tensor, this is because we set `output_sequence_length=15`, meaning no matter the size of the sequence we pass to `text_vectorizer`, it always returns a sequence with a length of 15.\n"]},{"cell_type":"code","metadata":{"id":"SZFka4BtRR6_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3b36632-1331-4ef0-c45e-ac17b0ba786b","executionInfo":{"status":"ok","timestamp":1660684478250,"user_tz":300,"elapsed":116,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Choose a random sentence from the training dataset and tokenize it\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\nVectorized version:\")\n","text_vectorizer([random_sentence])"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n","@RonWyden Democrats restricted  Blacks from Voting. In 48' Landslide Lyndon Johnson won Senate Election by 67 Votes of Dead People in Texas!      \n","\n","Vectorized version:\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n","array([[8987, 5841, 4745,    1,   20, 4294,    4, 3206,  374,    1, 3717,\n","        1449, 2711, 3033,   18]])>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["* `get_vocabulary` - method that allows for the retrieval of all the unique words in an object after it has been mapped into a number form (e.g. after it has been passed through `TextVectorization`)"],"metadata":{"id":"aywIEJNqiB5s"}},{"cell_type":"code","metadata":{"id":"5nwNdgAZIhna","colab":{"base_uri":"https://localhost:8080/"},"outputId":"914a7799-bb62-4aa9-f797-b5e5f5a9b700","executionInfo":{"status":"ok","timestamp":1660684486690,"user_tz":300,"elapsed":135,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get the unique words in the vocabulary\n","words_in_vocab = text_vectorizer.get_vocabulary()\n","top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n","bottom_5_words = words_in_vocab[-5:] # least common tokens\n","print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\") \n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in vocab: 10000\n","Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n","Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"]}]},{"cell_type":"markdown","metadata":{"id":"AHyCdO0uEOkH"},"source":["### Creating an Embedding using an Embedding Layer\n","\n","The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. `1` = I, `2` = love, `3` = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n","\n","We can see what an embedding of a word looks like by using the [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer. \n","\n","The main parameters we're concerned about here are:\n","* `input_dim` - The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).\n","* `output_dim` - The size of the output embedding vector, for example, a value of `100` outputs a  feature vector of size 100 for each word.\n","* `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n","* `input_length` - Length of sequences being passed to embedding layer.\n","\n"]},{"cell_type":"code","metadata":{"id":"OsB4StymSk_s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a06069ab-a694-43df-a826-1c84e30204a8","executionInfo":{"status":"ok","timestamp":1660684490153,"user_tz":300,"elapsed":120,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","\n","embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n","                             output_dim=128, # set size of embedding vector\n","                             embeddings_initializer=\"uniform\", # default, intialize randomly\n","                             input_length=max_length, # how long is each input\n","                             name=\"embedding_1\") \n","\n","embedding"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.layers.embeddings.Embedding at 0x7f6840100290>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"bfML_IzlSUho"},"source":[" `embedding` is a TensoFlow layer\n","* This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns.\n","\n","Let's try it out on a sample sentence"]},{"cell_type":"code","metadata":{"id":"1Re6Eew6SZnG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"17310d3c-455f-4545-f917-f20609d90f14","executionInfo":{"status":"ok","timestamp":1660684492974,"user_tz":300,"elapsed":264,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get a random sentence from training set\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\nEmbedded version:\")\n","\n","# Embed the random sentence (turn it into numerical representation)\n","sample_embed = embedding(text_vectorizer([random_sentence]))\n","sample_embed"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n","Breast milk is the original #superfood but rates worldwide have stalled below 40% contributing to more than 800000 child deaths last year.      \n","\n","Embedded version:\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n","array([[[ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n","          0.03332629,  0.02803668],\n","        [-0.03907432, -0.0302568 , -0.01059837, ...,  0.04604281,\n","          0.0300116 ,  0.02912701],\n","        [-0.03048784,  0.04003957,  0.01426423, ...,  0.04380543,\n","         -0.01184901,  0.04811785],\n","        ...,\n","        [-0.04519074, -0.04892057, -0.00671414, ...,  0.01201805,\n","          0.00584801,  0.01353866],\n","        [ 0.0233913 ,  0.02774353,  0.04601837, ..., -0.00401249,\n","          0.02798238,  0.01142389],\n","        [-0.03521683, -0.00981399, -0.00284491, ..., -0.04792733,\n","         -0.01664252, -0.03293632]]], dtype=float32)>"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"e4Sn8o9pTBE5"},"source":["Each token in the sentence gets turned into a length 128 feature vector."]},{"cell_type":"code","metadata":{"id":"g_VBepuSTBDW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"568bbf28-ee81-4adc-944a-a80551e72381","executionInfo":{"status":"ok","timestamp":1660684495104,"user_tz":300,"elapsed":141,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check out a single token's embedding\n","sample_embed[0][0]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(128,), dtype=float32, numpy=\n","array([ 0.03977952, -0.03782602, -0.03646283, -0.02449075, -0.00015752,\n","        0.02220254,  0.00162981,  0.00603487,  0.0085157 , -0.02620113,\n","        0.04101599,  0.03715892,  0.02397566,  0.00281113, -0.02704906,\n","       -0.04870148,  0.01457943,  0.0059551 , -0.02334484,  0.03581132,\n","        0.04377897,  0.04186075,  0.03245703, -0.045092  ,  0.04260418,\n","        0.03398135, -0.01812425, -0.03539513,  0.02954218,  0.02556742,\n","       -0.03345481,  0.04272738, -0.00798845, -0.0406163 , -0.00644834,\n","        0.00232404,  0.01703629,  0.03645121, -0.02622857,  0.03498118,\n","       -0.03059715,  0.02576998, -0.04221511,  0.02654583, -0.02192564,\n","       -0.0346157 ,  0.00075326,  0.01427345,  0.01027539, -0.04311384,\n","       -0.03973336, -0.00966626,  0.01032177, -0.04011822, -0.018892  ,\n","       -0.01233201,  0.02721632, -0.01232889, -0.02504088, -0.04715574,\n","        0.00558523, -0.00801403,  0.03058865, -0.01923352, -0.04175536,\n","       -0.03654208, -0.04277095, -0.0033918 , -0.04226271,  0.0240727 ,\n","        0.01274442, -0.00268712,  0.03158386,  0.04823284,  0.02940297,\n","        0.00636031,  0.01719667,  0.03907609, -0.0249153 , -0.00834242,\n","        0.03881217, -0.04461795,  0.03740455, -0.02412283, -0.01538421,\n","        0.03984089, -0.04073881,  0.0317078 , -0.04241145, -0.03435747,\n","        0.03021734,  0.0443267 ,  0.00424304, -0.01204874, -0.00213076,\n","       -0.02668054,  0.0461436 , -0.04644201,  0.02519932, -0.04503984,\n","       -0.03163396, -0.02846084,  0.01276297,  0.02938429,  0.03437588,\n","        0.00973482, -0.01860742,  0.03510927, -0.04996962,  0.01983938,\n","        0.00642185,  0.00372197,  0.01576127,  0.01591486, -0.04195998,\n","        0.02777341,  0.00619875, -0.02269079, -0.00224097,  0.0289592 ,\n","        0.04377018, -0.03377642,  0.00907839, -0.01134553, -0.02910458,\n","        0.00236253,  0.03332629,  0.02803668], dtype=float32)>"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"Z0NTsDklR0xw"},"source":["These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary.\n","\n","> 🔑 **Note:** The previous two concepts (tokenization and embeddings) are the foundation for many NLP tasks. So if you're not sure about anything, be sure to research and conduct your own experiments to further help your understanding."]},{"cell_type":"markdown","metadata":{"id":"ZJENUdF3F7Rn"},"source":["## Modelling a text dataset\n","\n","Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.\n","\n","To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.\n","\n","We'll be building the following:\n","* **Model 0**: Naive Bayes (baseline)\n","* **Model 1**: Feed-forward neural network (dense model)\n","* **Model 2**: LSTM model\n","* **Model 3**: GRU model\n","* **Model 4**: Bidirectional-LSTM model\n","* **Model 5**: 1D Convolutional Neural Network\n","* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n","* **Model 7**: Same as model 6 with 10% of training data\n","\n","Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n","\n","Each experiment will go through the following steps:\n","* Construct the model\n","* Train the model\n","* Make predictions with the model\n","* Track prediction evaluation metrics for later comparison\n"]},{"cell_type":"markdown","metadata":{"id":"q4i5BiQfF--y"},"source":["### Model 0: Getting a baseline\n","\n","As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n","\n","To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This was chosen via referring to the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n","\n","> 📖 **Reading:** The ins and outs of TF-IDF algorithm is beyond the scope of this notebook, however, the curious reader is encouraged to check out the [Scikit-Learn documentation for more](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)."]},{"cell_type":"code","metadata":{"id":"xFqjqWcXtOOs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe7507a3-131e-45aa-9a6e-8c27898d887e","executionInfo":{"status":"ok","timestamp":1660684501700,"user_tz":300,"elapsed":258,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","\n","# Create tokenization and modelling pipeline\n","model_0 = Pipeline([\n","                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n","                    (\"clf\", MultinomialNB()) # model the text\n","])\n","\n","# Fit the pipeline to the training data\n","model_0.fit(train_sentences, train_labels)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"ybOvOuVJbNjg"},"source":["The benefit of using a shallow model like Multinomial Naive Bayes is that training is very fast."]},{"cell_type":"code","metadata":{"id":"soPfnpmQuUIP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aea50d12-b713-4489-9f46-a9300d08cc93","executionInfo":{"status":"ok","timestamp":1660684503363,"user_tz":300,"elapsed":115,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# evaluate our model and find our baseline metric\n","baseline_score = model_0.score(val_sentences, val_labels)\n","print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Our baseline model achieves an accuracy of: 79.27%\n"]}]},{"cell_type":"code","metadata":{"id":"7n89JxrJufcf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7dbb7cff-2052-4069-b9be-344d894bc99d","executionInfo":{"status":"ok","timestamp":1660684504870,"user_tz":300,"elapsed":117,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions\n","baseline_preds = model_0.predict(val_sentences)\n","baseline_preds[:20]"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"K354svk_bmdf"},"source":["### Creating an evaluation function for our model experiments\n","\n","We could evaluate these as they are but since we're going to be evaluating several models in the same way going forward, let's create a helper function which takes an array of predictions and ground truth labels and computes the following:\n","* Accuracy\n","* Precision\n","* Recall\n","* F1-score\n","\n","> 🔑 **Note:** Since we're dealing with a classification problem, the above metrics are the most appropriate. If we were working with a regression problem, other metrics such as MAE (mean absolute error) would be a better choice."]},{"cell_type":"code","metadata":{"id":"gLmNlDjIxGgJ","executionInfo":{"status":"ok","timestamp":1660684514108,"user_tz":300,"elapsed":116,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Function to evaluate: accuracy, precision, recall, f1-score\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","def calculate_results(y_true, y_pred):\n","  \"\"\"\n","  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n","\n","  Args:\n","  -----\n","  y_true = true labels in the form of a 1D array\n","  y_pred = predicted labels in the form of a 1D array\n","\n","  Returns a dictionary of accuracy, precision, recall, f1-score.\n","\n","  \"\"\"\n","  # Calculate model accuracy\n","  model_accuracy = accuracy_score(y_true, y_pred)*100 \n","  # Calculate model precision, recall and f1 score using \"weighted\" average\n","  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n","  model_results = {\"accuracy\": model_accuracy,\n","                  \"precision\": model_precision*100,\n","                  \"recall\": model_recall*100,\n","                  \"f1\": model_f1*100}\n","  return model_results"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sgy1omMhwr52","colab":{"base_uri":"https://localhost:8080/"},"outputId":"082d4a58-5cf4-489e-d65b-dbef18414726","executionInfo":{"status":"ok","timestamp":1660684516250,"user_tz":300,"elapsed":118,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get baseline results\n","baseline_results = calculate_results(y_true=val_labels,\n","                                     y_pred=baseline_preds)\n","baseline_results"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 79.26509186351706,\n"," 'precision': 81.11390004213173,\n"," 'recall': 79.26509186351706,\n"," 'f1': 78.6218975804955}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"noRJNm7dGNyh"},"source":["### Model 1: A simple dense model\n","\n","The first \"deep\" model we're going to build is a single layer dense model. \n","\n","Model will:\n","1. Take our text and labels as input\n","2. Tokenize the text \n","3. Create an embedding\n","4. Find the average of the embedding (using Global Average Pooling) \n","5. Pass the average through a fully connected layer with one output unit and a sigmoid activation function\n","\n",">We'll import our `create_tensorboard_callback()` function from `helper_functions.py` to keep track of the results of each. "]},{"cell_type":"code","metadata":{"id":"PVMPUd3HTit5","executionInfo":{"status":"ok","timestamp":1660684518872,"user_tz":300,"elapsed":122,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Create tensorboard callback (need to create a new one for each model)\n","from helper_functions import create_tensorboard_callback\n","\n","# Create directory to save TensorBoard logs\n","SAVE_DIR = \"model_logs\""],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_rVtJA7yVBI","executionInfo":{"status":"ok","timestamp":1660684520794,"user_tz":300,"elapsed":3,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Build model with the Functional API\n","from tensorflow.keras import layers\n","\n","#build architecture \n","input_layer = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n","encoding_layer = text_vectorizer(input_layer) # turn the input text into numbers\n","embed_layer = embedding(encoding_layer) # create an embedding of the numerized numbers\n","pool_layer = layers.GlobalAveragePooling1D()(embed_layer) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n","output_layer = layers.Dense(1, activation=\"sigmoid\")(pool_layer) # create the output layer, want binary outputs so use sigmoid activation\n","\n","#create model instance\n","model_1 = tf.keras.Model(input_layer, output_layer, name=\"model_1_dense\") # construct the model"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYzsu36Y8JUe"},"source":["`model_1` \n","* takes a 1-dimensional string as input (in our case, a Tweet)\n","* tokenizes the string using `text_vectorizer` and creates an embedding using `embedding`.\n","* (optionally) pool the outputs of the embedding layer to reduce the dimensionality of the tensor we pass to the output layer.\n","* pass the output of the pooling layer to a dense layer with sigmoid activation (binary classification).\n","\n","> 🛠 **Exercise:** Try building `model_1` with and without a `GlobalAveragePooling1D()` layer after the `embedding` layer. What happens? Why do you think this is?\n"]},{"cell_type":"code","metadata":{"id":"Ubq0ctLD8CQq","executionInfo":{"status":"ok","timestamp":1660684523202,"user_tz":300,"elapsed":121,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compile model\n","model_1.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkJa-t8aTw1H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a55ec55f-8fc3-4596-bc66-d7c16d934110","executionInfo":{"status":"ok","timestamp":1660684524109,"user_tz":300,"elapsed":149,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get a summary of the model\n","model_1.summary()"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1_dense\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_1 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," global_average_pooling1d (G  (None, 128)              0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,280,129\n","Trainable params: 1,280,129\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"bH0JLyR09yYt"},"source":["* Most of the trainable parameters are contained within the embedding layer. \n",">Recall we created an embedding of size 128 (`output_dim=128`) for a vocabulary of size 10,000 (`input_dim=10000`), hence the 1,280,000 trainable parameters.\n","\n"]},{"cell_type":"code","metadata":{"id":"1YRYpJIfTvHV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5319d382-9be5-4c9a-a973-067046ad0a8a","executionInfo":{"status":"ok","timestamp":1660684535378,"user_tz":300,"elapsed":9094,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit the model\n","model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n","                                                                     experiment_name=\"simple_dense_model\")])"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/simple_dense_model/20220816-211526\n","Epoch 1/5\n","215/215 [==============================] - 5s 5ms/step - loss: 0.6094 - accuracy: 0.6916 - val_loss: 0.5357 - val_accuracy: 0.7572\n","Epoch 2/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.4410 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n","Epoch 3/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8605 - val_loss: 0.4590 - val_accuracy: 0.7900\n","Epoch 4/5\n","215/215 [==============================] - 1s 4ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4641 - val_accuracy: 0.7927\n","Epoch 5/5\n","215/215 [==============================] - 1s 5ms/step - loss: 0.2380 - accuracy: 0.9118 - val_loss: 0.4767 - val_accuracy: 0.7874\n"]}]},{"cell_type":"code","metadata":{"id":"zSTS87YGzuBG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0dac582-9bb8-44b8-e463-798a867cc997","executionInfo":{"status":"ok","timestamp":1660684538939,"user_tz":300,"elapsed":272,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check the results\n","model_1.evaluate(val_sentences, val_labels)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 0s 3ms/step - loss: 0.4767 - accuracy: 0.7874\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4766846001148224, 0.787401556968689]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5M2CTAetBVfW","outputId":"501d3965-b4ef-4259-acd2-9f15a3a86ecb","executionInfo":{"status":"ok","timestamp":1660684540065,"user_tz":300,"elapsed":124,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["embedding.weights"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n"," array([[ 0.00073165,  0.01504799, -0.03425454, ..., -0.04403538,\n","         -0.01042281,  0.01876437],\n","        [ 0.04135862, -0.03945086, -0.03811939, ...,  0.00464736,\n","          0.03163552,  0.02928301],\n","        [ 0.0068403 ,  0.05363134, -0.00241555, ..., -0.07082173,\n","         -0.04750705,  0.01448254],\n","        ...,\n","        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n","          0.00308807,  0.02215792],\n","        [ 0.00692343,  0.05942352, -0.01975194, ..., -0.06199061,\n","         -0.01018393,  0.03510419],\n","        [-0.0372346 ,  0.06267187, -0.07451148, ..., -0.02367218,\n","         -0.0864333 ,  0.01742156]], dtype=float32)>]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3rfhJFSBrga","outputId":"d6d1fb01-0fb4-4b0c-93cd-2fb936c70f27","executionInfo":{"status":"ok","timestamp":1660684542111,"user_tz":300,"elapsed":108,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n","print(embed_weights.shape)"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 128)\n"]}]},{"cell_type":"code","metadata":{"id":"t6UrSgRVU6pl","colab":{"base_uri":"https://localhost:8080/","height":148},"executionInfo":{"status":"error","timestamp":1660604284907,"user_tz":300,"elapsed":175,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"3ebf5fb9-ea6e-47ab-f17f-db5456f2b94a"},"source":["# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n","# # Upload TensorBoard dev records\n","# !tensorboard dev upload --logdir ./model_logs \\\n","#   --name \"First deep model on text data\" \\\n","#   --description \"Trying a dense model with an embedding layer\" \\\n","#   --one_shot # exits the uploader when upload has finished"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-573a19b7e159>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    get_ipython().system('tensorboard dev upload --logdir ./model_logs      --name \"First deep model on text data\"      --description \"Trying a dense model with an embedding layer\"      --one_shot # exits the uploader when upload has finished')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","metadata":{"id":"DVyJl-VE1ACz"},"source":["# If you need to remove previous experiments, you can do so using the following command\n"," !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5X7kbEmAzzxM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d49a747b-6d4f-491a-ef70-8f9542f93caf","executionInfo":{"status":"ok","timestamp":1660684546599,"user_tz":300,"elapsed":239,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions (these come back in the form of probabilities)\n","model_1_pred_probs = model_1.predict(val_sentences)\n","model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.40488204],\n","       [0.7443312 ],\n","       [0.997895  ],\n","       [0.10889999],\n","       [0.11143532],\n","       [0.93556094],\n","       [0.9134595 ],\n","       [0.9925345 ],\n","       [0.97156817],\n","       [0.26570344]], dtype=float32)"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"YWU5e1NLAKJ9"},"source":["Since our final layer uses a sigmoid activation function, we get our predictions back in the form of probabilities.\n","\n","To convert them to prediction classes, we'll use `tf.round()`, meaning prediction probabilities below 0.5 will be rounded to 0 and those above 0.5 will be rounded to 1.\n","\n","> 🔑 **Note:** In practice, the output threshold of a sigmoid prediction probability doesn't necessarily have to 0.5. For example, through testing, you may find that a cut off of 0.25 is better for your chosen evaluation metrics. A common example of this threshold cutoff is the [precision-recall tradeoff](https://www.machinelearningaptitude.com/topics/machine-learning/what-is-precision-recall-tradeoff/#:~:text=precision%2Drecall%20tradeoff%20occur%20due,the%20threshold%20of%20the%20classifier.&text=When%20threshold%20is%20decreased%20to,but%20precision%20decreases%20to%200.4.)."]},{"cell_type":"code","metadata":{"id":"Qf-R_1vsz47P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"626edda6-9486-46b1-a0ae-0f3835260b16","executionInfo":{"status":"ok","timestamp":1660684548920,"user_tz":300,"elapsed":109,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Turn prediction probabilities into single-dimension tensor of floats\n","model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n","model_1_preds[:20]"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n","       0., 0., 1.], dtype=float32)>"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"Zc3ryY0yCHcI"},"source":["Now we've got our model's predictions in the form of classes, we can use our `calculate_results()` function to compare them to the ground truth validation labels."]},{"cell_type":"code","metadata":{"id":"iDEEhYTF0X1y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4eecf798-e615-43f2-b604-a572bebc1ca0","executionInfo":{"status":"ok","timestamp":1660684552293,"user_tz":300,"elapsed":117,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate model_1 metrics\n","model_1_results = calculate_results(y_true=val_labels, \n","                                    y_pred=model_1_preds)\n","model_1_results"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 78.74015748031496,\n"," 'precision': 79.14920592553048,\n"," 'recall': 78.74015748031496,\n"," 'f1': 78.46966492209201}"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"gnkK6Uc7CYlX"},"source":["* Compare model_1 to  baseline model"]},{"cell_type":"code","metadata":{"id":"Jp88ystW1m0d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4f5afc3-2f28-4c0f-d1ab-1662f7a8459a","executionInfo":{"status":"ok","timestamp":1660684553894,"user_tz":300,"elapsed":133,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Is our simple Keras model better than our baseline model?\n","import numpy as np\n","np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False, False, False, False])"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"lUINrCdRCpFf"},"source":["Since we'll be doing this kind of comparison (baseline compared to new model) quite a few times, let's create a function to help us out. "]},{"cell_type":"code","metadata":{"id":"wo3norTG3GrE","executionInfo":{"status":"ok","timestamp":1660684555791,"user_tz":300,"elapsed":117,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Create a helper function to compare our baseline results to new model results\n","def compare_baseline_to_new_results(baseline_results, new_model_results):\n","  for key, value in baseline_results.items():\n","    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n"],"execution_count":41,"outputs":[]},{"cell_type":"code","source":["#compare models\n","compare_baseline_to_new_results(baseline_results=baseline_results, \n","                                new_model_results=model_1_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-7Ly0LBvZLW","executionInfo":{"status":"ok","timestamp":1660684557764,"user_tz":300,"elapsed":3,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"4a4ff9d8-4a27-4652-ad15-fc64eb965d1a"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n","Baseline precision: 81.11, New precision: 79.15, Difference: -1.96\n","Baseline recall: 79.27, New recall: 78.74, Difference: -0.52\n","Baseline f1: 78.62, New f1: 78.47, Difference: -0.15\n"]}]},{"cell_type":"markdown","metadata":{"id":"6e-1LuioSLAM"},"source":["## Visualizing learned embeddings\n","\n","Our first model (`model_1`) contained an embedding layer (`embedding`) which learned a way of representing words as feature vectors by passing over the training data.\n","\n","To further help understand what a text embedding is, let's visualize the embedding our model learned.\n","\n","To do so, let's remind ourselves of the words in our vocabulary.\n"]},{"cell_type":"code","metadata":{"id":"-DkcfRQBVXuJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"522c86a6-1585-45ff-8339-ac4a5aec8273","executionInfo":{"status":"ok","timestamp":1660684560328,"user_tz":300,"elapsed":122,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get the vocabulary from the text vectorization layer\n","words_in_vocab = text_vectorizer.get_vocabulary()\n","len(words_in_vocab), words_in_vocab[:10]"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"8EUR9PwrZphh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"241bc6a5-cd84-48bf-843f-2b371e1f41ab","executionInfo":{"status":"ok","timestamp":1660684561264,"user_tz":300,"elapsed":142,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["model_1.summary()"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1_dense\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_1 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," global_average_pooling1d (G  (None, 128)              0         \n"," lobalAveragePooling1D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,280,129\n","Trainable params: 1,280,129\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"9xJ5LrInWDLo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a8e4e62-eb72-4a7d-da93-e8b0ecfac5b9","executionInfo":{"status":"ok","timestamp":1660684563286,"user_tz":300,"elapsed":103,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get the weight matrix of embedding layer \n","# (these are the numerical patterns between the text in the training dataset the model has learned)\n","embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n","print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 128)\n"]}]},{"cell_type":"markdown","metadata":{"id":"jzOJhJHPW1ju"},"source":["Now we've got these two objects, we can use the [Embedding Projector tool](http://projector.tensorflow.org/_) to visualize our embedding. \n","\n","To use the Embedding Projector tool, we need two files:\n","* The embedding vectors (same as embedding weights).\n","* The meta data of the embedding vectors (the words they represent - our vocabulary).\n","\n","Right now, we've got of these files as Python objects. To download them to file, we're going to [use the code example available on the TensorFlow word embeddings tutorial page](https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk).\n"]},{"cell_type":"code","metadata":{"id":"4e9rfcK6WxQE","executionInfo":{"status":"ok","timestamp":1660684565628,"user_tz":300,"elapsed":1033,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n","import io\n","\n","# Create output writers-->need these to use the embedding projector tool\n","out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n","out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n","\n","# Write embedding vectors and words to file\n","for num, word in enumerate(words_in_vocab):\n","   if num == 0: \n","      continue # skip padding token\n","   vec = embed_weights[num]\n","   out_m.write(word + \"\\n\") # write words to file\n","   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n","out_v.close()\n","out_m.close()"],"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# Download files locally to upload to Embedding Projector\n","try:\n","  from google.colab import files\n","except ImportError:\n","   pass\n","else:\n","  files.download(\"embedding_vectors.tsv\")\n","  files.download(\"embedding_metadata.tsv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"DF7SV4BwxfG_","executionInfo":{"status":"ok","timestamp":1660684579367,"user_tz":300,"elapsed":132,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"0e77d1c3-7b29-4cb3-f364-347e4fd766e3"},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a21fee65-f567-4631-9d90-151e9679a592\", \"embedding_vectors.tsv\", 15390766)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_d2d6b3d8-bef5-4d59-964a-02bd0a145ec0\", \"embedding_metadata.tsv\", 80388)"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"BVM7ifzpZaxJ"},"source":["Once you've downloaded the embedding vectors and metadata, you can visualize them using Embedding Vector tool:\n","1. Go to  http://projector.tensorflow.org/\n","2. Click on \"Load data\"\n","3. Upload the two files you downloaded (`embedding_vectors.tsv` and `embedding_metadata.tsv`)\n","4. Explore\n","5. Optional: You can share the data you've created by clicking \"Publish\"\n","\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"ON6AE3CLhWA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDERKwP_XWro"},"source":["### Model 2: LSTM\n","\n","To harness the power of the LSTM, we'll use [`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM).\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-RNN-architecture-coloured-block-edition.png)\n","\n","\n","model 2 is going to take on a very similar structure to `model_1`:\n","\n","```\n","Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n","```\n","\n","The main difference will be that we're going to add an LSTM layer between our embedding and output.\n","\n","* To make sure we're not getting reusing trained embeddings (this would involve data leakage between models, leading to an uneven comparison later on), we'll create another embedding layer (`model_2_embedding`) for our model. \n","* The `text_vectorizer` layer can be reused since it doesn't get updated during training.\n","\n","> 🔑 **Note:** The reason we use a new embedding layer for each model is since the embedding layer is a *learned* representation of words (as numbers), if we were to use the same embedding layer (`embedding_1`) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea."]},{"cell_type":"code","metadata":{"id":"Pi3vjpFU46hi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18136d13-e7d5-4938-b266-f7421d1b19de","executionInfo":{"status":"ok","timestamp":1660685297366,"user_tz":300,"elapsed":668,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Set random seed and create embedding layer (new embedding layer for each model)\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_2\")\n","\n","\n","# Create LSTM model\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_2_embedding(x)\n","print(x.shape)\n","#x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n","#print(x.shape)\n","x = layers.LSTM(64)(x) # return vector for whole sequence\n","print(x.shape)\n","# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 15, 128)\n","(None, 64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"e1wfTARuwWDg"},"source":["> 🔑 **Note:** Reading the documentation for the [TensorFlow LSTM layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), you'll find a plethora of parameters. Many of these have been tuned to make sure they compute as fast as possible. The main ones you'll be looking to adjust are `units` (number of hidden units) and `return_sequences` (set this to `True` when stacking LSTM or other recurrent layers).\n","\n","Now we've got our LSTM model built, let's compile it using `\"binary_crossentropy\"` loss and the Adam optimizer."]},{"cell_type":"code","metadata":{"id":"pWdt3bFRwG6w","executionInfo":{"status":"ok","timestamp":1660685385730,"user_tz":300,"elapsed":120,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compile model\n","model_2.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAjdfDfLwK_R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b19f2d10-79be-4c70-dd01-36380379501b","executionInfo":{"status":"ok","timestamp":1660685390122,"user_tz":300,"elapsed":146,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["model_2.summary()"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2_LSTM\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_2 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," lstm_6 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1,329,473\n","Trainable params: 1,329,473\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"S5NLw3wD0aMz"},"source":["Looking good! You'll notice a fair few more trainable parameters within our LSTM layer than `model_1`. \n","\n","If you'd like to know where this number comes from, I recommend going through the above resources as well the following on calculating the number of parameters in an LSTM cell:\n","* [Stack Overflow answer to calculate the number of parameters in an LSTM cell](https://stackoverflow.com/questions/38080035/how-to-calculate-the-number-of-parameters-of-an-lstm-network) by Marcin Możejko\n","* [Calculating number of parameters in a LSTM unit and layer](https://medium.com/@priyadarshi.cse/calculating-number-of-parameters-in-a-lstm-unit-layer-7e491978e1e4) by Shridhar Priyadarshi\n","\n","Now our first RNN model's compiled let's fit it to our training data, validating it on the validation data and tracking its training parameters using our TensorBoard callback."]},{"cell_type":"code","metadata":{"id":"YgZ7ojDvwKcq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97022a77-d89e-4e0a-fbad-959d677f94ca","executionInfo":{"status":"ok","timestamp":1660685427666,"user_tz":300,"elapsed":22442,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit model\n","model_2_history = model_2.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n","                                                                     \"LSTM\")])"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/LSTM/20220816-213005\n","Epoch 1/5\n","215/215 [==============================] - 6s 9ms/step - loss: 0.5100 - accuracy: 0.7416 - val_loss: 0.4566 - val_accuracy: 0.7822\n","Epoch 2/5\n","215/215 [==============================] - 2s 7ms/step - loss: 0.3176 - accuracy: 0.8717 - val_loss: 0.5138 - val_accuracy: 0.7756\n","Epoch 3/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.2201 - accuracy: 0.9152 - val_loss: 0.5858 - val_accuracy: 0.7677\n","Epoch 4/5\n","215/215 [==============================] - 1s 7ms/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.6041 - val_accuracy: 0.7743\n","Epoch 5/5\n","215/215 [==============================] - 1s 7ms/step - loss: 0.1076 - accuracy: 0.9594 - val_loss: 0.8746 - val_accuracy: 0.7507\n"]}]},{"cell_type":"code","metadata":{"id":"4c_lVbKLemrU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8278d185-a50a-49f3-db04-9bcae8f6c978","executionInfo":{"status":"ok","timestamp":1660685432523,"user_tz":300,"elapsed":1079,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions on the validation dataset\n","model_2_pred_probs = model_2.predict(val_sentences)\n","model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((762, 1), array([[0.007126  ],\n","        [0.78736776],\n","        [0.9996376 ],\n","        [0.0567917 ],\n","        [0.0025822 ],\n","        [0.9996238 ],\n","        [0.92170197],\n","        [0.9997993 ],\n","        [0.9994954 ],\n","        [0.6645739 ]], dtype=float32))"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"fQ6ope-ddpOo"},"source":["We can turn these prediction probabilities into prediction classes by rounding to the nearest integer (by default, prediction probabilities under 0.5 will go to 0 and those over 0.5 will go to 1)."]},{"cell_type":"code","metadata":{"id":"iFnIhtyE7hlb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a7c665f-1b85-4861-d4d9-99c563bccc9c","executionInfo":{"status":"ok","timestamp":1660685444656,"user_tz":300,"elapsed":135,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Round out predictions and reduce to 1-dimensional array\n","model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n","model_2_preds[:10]"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"3iHXv04y76vj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d47adb0-f5d7-48d4-9055-1b22e2f24817","executionInfo":{"status":"ok","timestamp":1660685457422,"user_tz":300,"elapsed":141,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate LSTM model results\n","model_2_results = calculate_results(y_true=val_labels,\n","                                    y_pred=model_2_preds)\n","model_2_results"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 75.06561679790026,\n"," 'precision': 75.10077975908163,\n"," 'recall': 75.06561679790026,\n"," 'f1': 74.89268622514025}"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"ZdQGn2L68B5Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"03198790-8565-4452-929f-e824f3e4806d","executionInfo":{"status":"ok","timestamp":1660685465928,"user_tz":300,"elapsed":131,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compare model 2 to baseline\n","compare_baseline_to_new_results(baseline_results, model_2_results)"],"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n","Baseline precision: 81.11, New precision: 75.10, Difference: -6.01\n","Baseline recall: 79.27, New recall: 75.07, Difference: -4.20\n","Baseline f1: 78.62, New f1: 74.89, Difference: -3.73\n"]}]},{"cell_type":"markdown","metadata":{"id":"Q0pAtADt8ju7"},"source":["### Model 3: GRU\n","\n","Another popular and effective RNN component is the GRU or gated recurrent unit.\n","\n","The GRU cell has similar features to an LSTM cell but has less parameters.\n","\n","> 📖 **Resource:** A full explanation of the GRU cell is beyond the scope of this noteook but I'd suggest the following resources to learn more:\n","* [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) Wikipedia page\n","* [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be) by Simeon Kostadinov\n","\n","To use the GRU cell in TensorFlow, we can call the [`tensorflow.keras.layers.GRU()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) class.\n","\n","The architecture of the GRU-powered model will follow the same structure we've been using:\n","\n","```\n","Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n","```\n","\n","Again, the only difference will be the layer(s) we use between the embedding and the output."]},{"cell_type":"code","metadata":{"id":"SoSCGq3H47Yo","executionInfo":{"status":"ok","timestamp":1660686701032,"user_tz":300,"elapsed":338,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Set random seed and create embedding layer (new embedding layer for each model)\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_3\")\n","\n","# Build an RNN using the GRU cell\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_3_embedding(x)\n","# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\n","x = layers.GRU(64)(x) \n","# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBL1mb31hHDS","executionInfo":{"status":"ok","timestamp":1660686702465,"user_tz":300,"elapsed":152,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compile GRU model\n","model_3.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVnB5yQeiAWs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c6edc99-7cf6-49a0-9108-b59690774c41","executionInfo":{"status":"ok","timestamp":1660686704091,"user_tz":300,"elapsed":148,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get a summary of the GRU model\n","model_3.summary()"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3_GRU\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_8 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_3 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," gru (GRU)                   (None, 64)                37248     \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 1,317,313\n","Trainable params: 1,317,313\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"KcXzKqgXhdez"},"source":["Notice the difference in number of trainable parameters between `model_2` (LSTM) and `model_3` (GRU). The difference comes from the LSTM cell having more trainable parameters than the GRU cell.\n","\n","We'll fit our model just as we've been doing previously. We'll also track our models results using our `create_tensorboard_callback()` function."]},{"cell_type":"code","metadata":{"id":"Gvamg5JOh_jC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eeb358a2-81d1-4dc7-8144-cfd1bdcc5a41","executionInfo":{"status":"ok","timestamp":1660686733041,"user_tz":300,"elapsed":12561,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit model\n","model_3_history = model_3.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")])"],"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/GRU/20220816-215200\n","Epoch 1/5\n","215/215 [==============================] - 4s 9ms/step - loss: 0.5242 - accuracy: 0.7314 - val_loss: 0.4553 - val_accuracy: 0.7769\n","Epoch 2/5\n","215/215 [==============================] - 2s 7ms/step - loss: 0.3195 - accuracy: 0.8694 - val_loss: 0.4937 - val_accuracy: 0.7808\n","Epoch 3/5\n","215/215 [==============================] - 2s 8ms/step - loss: 0.2197 - accuracy: 0.9181 - val_loss: 0.5607 - val_accuracy: 0.7743\n","Epoch 4/5\n","215/215 [==============================] - 3s 12ms/step - loss: 0.1599 - accuracy: 0.9441 - val_loss: 0.6220 - val_accuracy: 0.7782\n","Epoch 5/5\n","215/215 [==============================] - 3s 13ms/step - loss: 0.1221 - accuracy: 0.9584 - val_loss: 0.6205 - val_accuracy: 0.7677\n"]}]},{"cell_type":"code","metadata":{"id":"W5TUVHCl9pe-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"974965e0-0b5b-474e-ce6e-a1593e7d746e","executionInfo":{"status":"ok","timestamp":1660686830842,"user_tz":300,"elapsed":446,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions on the validation data\n","model_3_pred_probs = model_3.predict(val_sentences)\n","model_3_pred_probs.shape, model_3_pred_probs[:10]"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((762, 1), array([[0.33325282],\n","        [0.87741184],\n","        [0.9980252 ],\n","        [0.11561754],\n","        [0.01235959],\n","        [0.9925639 ],\n","        [0.62142634],\n","        [0.99813336],\n","        [0.9982377 ],\n","        [0.50181067]], dtype=float32))"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"haILbddg98CY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f74c9dbe-7a5d-4690-f786-1de601aa69c7","executionInfo":{"status":"ok","timestamp":1660686835940,"user_tz":300,"elapsed":114,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Convert prediction probabilities to prediction classes\n","model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n","model_3_preds[:10]"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"h9OZbQu1-LPp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4925a26f-fddc-474f-e449-652a9d4493a2","executionInfo":{"status":"ok","timestamp":1660686837983,"user_tz":300,"elapsed":119,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calcuate model_3 results\n","model_3_results = calculate_results(y_true=val_labels, \n","                                    y_pred=model_3_preds)\n","model_3_results"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 76.77165354330708,\n"," 'precision': 76.75450859410361,\n"," 'recall': 76.77165354330708,\n"," 'f1': 76.67932666650168}"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"_7AE6vtn-RQZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8ae5cdb-8aeb-4919-b35b-83caef438195","executionInfo":{"status":"ok","timestamp":1660686839516,"user_tz":300,"elapsed":167,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compare to baseline\n","compare_baseline_to_new_results(baseline_results, model_3_results)"],"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\n","Baseline precision: 81.11, New precision: 76.75, Difference: -4.36\n","Baseline recall: 79.27, New recall: 76.77, Difference: -2.49\n","Baseline f1: 78.62, New f1: 76.68, Difference: -1.94\n"]}]},{"cell_type":"markdown","metadata":{"id":"oLm6r4nQ-Wdr"},"source":["### Model 4: Bidirectonal RNN model \n","\n","A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left."]},{"cell_type":"code","metadata":{"id":"NAU9dvGm47_2","executionInfo":{"status":"ok","timestamp":1660687377897,"user_tz":300,"elapsed":658,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Set random seed and create embedding layer (new embedding layer for each model)\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","model_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_4\")\n","\n","# Build a Bidirectional RNN in TensorFlow\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_4_embedding(x)\n","# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n","x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","#create model instance \n","model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Hm5cwmNm-g4"},"source":["> 🔑 **Note:** You can use the `Bidirectional` wrapper on any RNN cell in TensorFlow. For example, `layers.Bidirectional(layers.GRU(64))` creates a bidirectional GRU cell.\n","\n","Our bidirectional model is built, let's compile it."]},{"cell_type":"code","metadata":{"id":"wP1jeF0am9x0","executionInfo":{"status":"ok","timestamp":1660687380226,"user_tz":300,"elapsed":118,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compile\n","model_4.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sUd9AQ6nFXI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"47d4a69e-63a9-4fd1-e556-e1d380ebb33c","executionInfo":{"status":"ok","timestamp":1660687381541,"user_tz":300,"elapsed":134,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get a summary of our bidirectional model\n","model_4.summary()"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4_Bidirectional\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_9 (InputLayer)        [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_4 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 128)              98816     \n"," l)                                                              \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,378,945\n","Trainable params: 1,378,945\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"TvItfzeZnIE-"},"source":["Notice the increased number of trainable parameters in `model_4` (bidirectional LSTM) compared to `model_2` (regular LSTM). This is due to the bidirectionality we added to our RNN.\n","\n","Time to fit our bidirectional model and track its performance."]},{"cell_type":"code","metadata":{"id":"bAKY_QbHXPHB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cc99c64a-cb0f-4a4b-b55a-7082b44d02b2","executionInfo":{"status":"ok","timestamp":1660687401054,"user_tz":300,"elapsed":16673,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit the model (takes longer because of the bidirectional layers)\n","model_4_history = model_4.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])"],"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/bidirectional_RNN/20220816-220304\n","Epoch 1/5\n","215/215 [==============================] - 9s 21ms/step - loss: 0.5093 - accuracy: 0.7481 - val_loss: 0.4606 - val_accuracy: 0.7795\n","Epoch 2/5\n","215/215 [==============================] - 2s 9ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.5144 - val_accuracy: 0.7690\n","Epoch 3/5\n","215/215 [==============================] - 2s 9ms/step - loss: 0.2150 - accuracy: 0.9178 - val_loss: 0.5626 - val_accuracy: 0.7677\n","Epoch 4/5\n","215/215 [==============================] - 2s 9ms/step - loss: 0.1523 - accuracy: 0.9469 - val_loss: 0.6365 - val_accuracy: 0.7769\n","Epoch 5/5\n","215/215 [==============================] - 2s 9ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.6509 - val_accuracy: 0.7664\n"]}]},{"cell_type":"code","metadata":{"id":"uFc7QHRtXmn7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3d4a4ca8-6272-4de1-99d6-32eb8ca9ba4d","executionInfo":{"status":"ok","timestamp":1660687493834,"user_tz":300,"elapsed":959,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions with bidirectional RNN on the validation data\n","model_4_pred_probs = model_4.predict(val_sentences)\n","model_4_pred_probs[:10]"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.04000054],\n","       [0.827929  ],\n","       [0.99842227],\n","       [0.13531098],\n","       [0.00311338],\n","       [0.99220747],\n","       [0.95528346],\n","       [0.99945647],\n","       [0.99898285],\n","       [0.28141677]], dtype=float32)"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"G5z8bMdaXw51","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d62ca0c-4a5c-4f51-e5b6-c959b5a2c1ed","executionInfo":{"status":"ok","timestamp":1660687495334,"user_tz":300,"elapsed":147,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Convert prediction probabilities to labels\n","model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n","model_4_preds[:10]"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"-a7Ym_vKYAO4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d681b6e0-a899-4031-f514-54703dc65860","executionInfo":{"status":"ok","timestamp":1660687497236,"user_tz":300,"elapsed":120,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate bidirectional RNN model results\n","model_4_results = calculate_results(val_labels, model_4_preds)\n","model_4_results"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 76.64041994750657,\n"," 'precision': 76.65895370389822,\n"," 'recall': 76.64041994750657,\n"," 'f1': 76.51213533864446}"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"hAET-LKpYT18","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d59d875-a7e9-4b75-e4c4-bc758f0a887a","executionInfo":{"status":"ok","timestamp":1660687498828,"user_tz":300,"elapsed":3,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check to see how the bidirectional model performs against the baseline\n","compare_baseline_to_new_results(baseline_results, model_4_results)"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 76.64, Difference: -2.62\n","Baseline precision: 81.11, New precision: 76.66, Difference: -4.45\n","Baseline recall: 79.27, New recall: 76.64, Difference: -2.62\n","Baseline f1: 78.62, New f1: 76.51, Difference: -2.11\n"]}]},{"cell_type":"markdown","metadata":{"id":"lgXEorf9GWY1"},"source":["### Model 5: Conv1D\n","\n","Before we build a full 1-dimensional CNN model, let's see a 1-dimensional convolutional layer (also called a **temporal convolution**) in action.\n","\n","We'll first create an embedding of a sample of text and experiment passing it through a `Conv1D()` layer and `GlobalMaxPool1D()` layer."]},{"cell_type":"code","metadata":{"id":"563hl7nPWP_3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ccd0f2c0-8016-4786-cc93-fad18c189bcd","executionInfo":{"status":"ok","timestamp":1660688951938,"user_tz":300,"elapsed":5885,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Test out the embedding, 1D convolutional and max pooling\n","embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sentence into embedding\n","conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time\n","conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer\n","max_pool = layers.GlobalMaxPool1D() \n","max_pool_output = max_pool(conv_1d_output) # get the most important features\n","embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"-WzTeShEemJ2"},"source":["Notice the output shapes of each layer.\n","\n","The embedding has an output shape dimension of the parameters we set it to (`input_length=15` and `output_dim=128`).\n","\n","The 1-dimensional convolutional layer has an output which has been compressed inline with its parameters. And the same goes for the max pooling layer output.\n","\n","Our text starts out as a string but gets converted to a feature vector of length 64 through various transformation steps (from tokenization to embedding to 1-dimensional convolution to max pool).\n","\n","Let's take a peak at what each of these transformations looks like."]},{"cell_type":"code","metadata":{"id":"gRcxYgs-dxM8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c56fe335-515e-45fe-b9af-648116ed1d58","executionInfo":{"status":"ok","timestamp":1660688987827,"user_tz":300,"elapsed":136,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# See the outputs of each layer\n","embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n"," array([[[ 0.02534914, -0.03109059,  0.00285616, ..., -0.0078316 ,\n","          -0.02685575, -0.04434131],\n","         [-0.0658626 ,  0.09451494, -0.01477602, ..., -0.00657781,\n","          -0.0423879 ,  0.07777896],\n","         [-0.04803652, -0.00709756, -0.02330893, ..., -0.01807331,\n","           0.02351034,  0.02676385],\n","         ...,\n","         [ 0.00073165,  0.01504799, -0.03425454, ..., -0.04403538,\n","          -0.01042281,  0.01876437],\n","         [ 0.00073165,  0.01504799, -0.03425454, ..., -0.04403538,\n","          -0.01042281,  0.01876437],\n","         [ 0.00073165,  0.01504799, -0.03425454, ..., -0.04403538,\n","          -0.01042281,  0.01876437]]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n"," array([[[0.08324981, 0.00648717, 0.        , 0.03983573, 0.        ,\n","          0.01144417, 0.00416249, 0.02288391, 0.        , 0.0090098 ,\n","          0.        , 0.        , 0.03401769, 0.06408274, 0.08103721,\n","          0.00409014, 0.01579618, 0.        , 0.07930179, 0.        ,\n","          0.        , 0.        , 0.14525083, 0.        , 0.        ,\n","          0.        , 0.03682077, 0.06534287, 0.        , 0.        ,\n","          0.05094624, 0.        ],\n","         [0.        , 0.0538719 , 0.        , 0.11491335, 0.        ,\n","          0.        , 0.1623708 , 0.        , 0.        , 0.00171254,\n","          0.1433671 , 0.        , 0.        , 0.        , 0.        ,\n","          0.01197932, 0.        , 0.        , 0.1355137 , 0.00401059,\n","          0.10309823, 0.09445542, 0.08390295, 0.        , 0.04213034,\n","          0.04487596, 0.06560459, 0.        , 0.02272683, 0.        ,\n","          0.        , 0.        ],\n","         [0.03683226, 0.04895762, 0.        , 0.15324754, 0.        ,\n","          0.        , 0.        , 0.        , 0.        , 0.        ,\n","          0.        , 0.04650312, 0.00496457, 0.07349403, 0.01608638,\n","          0.        , 0.0277912 , 0.        , 0.0808056 , 0.01403175,\n","          0.        , 0.03768814, 0.10382783, 0.        , 0.03361659,\n","          0.        , 0.02577607, 0.00140355, 0.        , 0.        ,\n","          0.032115  , 0.        ],\n","         [0.00887823, 0.10450976, 0.        , 0.06974533, 0.02328688,\n","          0.        , 0.04052208, 0.        , 0.        , 0.02733764,\n","          0.08674343, 0.        , 0.        , 0.06129853, 0.02007266,\n","          0.        , 0.        , 0.        , 0.03364262, 0.        ,\n","          0.04525334, 0.05219703, 0.06375711, 0.        , 0.        ,\n","          0.00774407, 0.0027347 , 0.        , 0.        , 0.00499632,\n","          0.        , 0.        ],\n","         [0.        , 0.0236907 , 0.        , 0.05827615, 0.05297642,\n","          0.        , 0.        , 0.        , 0.        , 0.        ,\n","          0.01719716, 0.02936819, 0.00466101, 0.06879887, 0.01944805,\n","          0.0158553 , 0.01294545, 0.        , 0.06866527, 0.        ,\n","          0.00623769, 0.03514052, 0.02407541, 0.        , 0.05979814,\n","          0.        , 0.01170144, 0.        , 0.        , 0.        ,\n","          0.04444933, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.05054971, 0.06105436,\n","          0.        , 0.00997431, 0.01403007, 0.        , 0.01680729,\n","          0.03148506, 0.03889384, 0.        , 0.0771068 , 0.00590969,\n","          0.        , 0.00263031, 0.        , 0.08935823, 0.        ,\n","          0.        , 0.05331151, 0.05227957, 0.        , 0.06658381,\n","          0.01881704, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.0200846 , 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.0505497 , 0.06105436,\n","          0.        , 0.0099743 , 0.01403007, 0.        , 0.01680729,\n","          0.03148505, 0.03889386, 0.        , 0.0771068 , 0.00590969,\n","          0.        , 0.00263032, 0.        , 0.08935823, 0.        ,\n","          0.        , 0.05331151, 0.05227958, 0.        , 0.06658381,\n","          0.01881704, 0.02448696, 0.        , 0.        , 0.        ,\n","          0.0200846 , 0.        ],\n","         [0.03544864, 0.        , 0.        , 0.0505497 , 0.06105436,\n","          0.        , 0.0099743 , 0.01403007, 0.        , 0.01680728,\n","          0.03148505, 0.03889384, 0.        , 0.0771068 , 0.00590969,\n","          0.        , 0.00263032, 0.        , 0.08935824, 0.        ,\n","          0.        , 0.05331151, 0.05227958, 0.        , 0.0665838 ,\n","          0.01881703, 0.02448696, 0.        , 0.        , 0.        ,\n","          0.02008461, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.0505497 , 0.06105436,\n","          0.        , 0.0099743 , 0.01403007, 0.        , 0.01680729,\n","          0.03148504, 0.03889385, 0.        , 0.0771068 , 0.00590969,\n","          0.        , 0.00263031, 0.        , 0.08935823, 0.        ,\n","          0.        , 0.05331151, 0.05227958, 0.        , 0.06658383,\n","          0.01881704, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.0200846 , 0.        ],\n","         [0.03544864, 0.        , 0.        , 0.05054972, 0.06105436,\n","          0.        , 0.00997431, 0.01403007, 0.        , 0.01680728,\n","          0.03148504, 0.03889384, 0.        , 0.0771068 , 0.0059097 ,\n","          0.        , 0.00263031, 0.        , 0.08935824, 0.        ,\n","          0.        , 0.05331151, 0.05227957, 0.        , 0.06658382,\n","          0.01881704, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.02008459, 0.        ],\n","         [0.03544863, 0.        , 0.        , 0.0505497 , 0.06105436,\n","          0.        , 0.00997431, 0.01403007, 0.        , 0.01680729,\n","          0.03148505, 0.03889384, 0.        , 0.0771068 , 0.00590969,\n","          0.        , 0.00263031, 0.        , 0.08935823, 0.        ,\n","          0.        , 0.05331151, 0.05227958, 0.        , 0.06658381,\n","          0.01881703, 0.02448697, 0.        , 0.        , 0.        ,\n","          0.0200846 , 0.        ]]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n"," array([[0.08324981, 0.10450976, 0.        , 0.15324754, 0.06105436,\n","         0.01144417, 0.1623708 , 0.02288391, 0.        , 0.02733764,\n","         0.1433671 , 0.04650312, 0.03401769, 0.0771068 , 0.08103721,\n","         0.0158553 , 0.0277912 , 0.        , 0.1355137 , 0.01403175,\n","         0.10309823, 0.09445542, 0.14525083, 0.        , 0.06658383,\n","         0.04487596, 0.06560459, 0.06534287, 0.02272683, 0.00499632,\n","         0.05094624, 0.        ]], dtype=float32)>)"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"G9aphPWCYkWN","executionInfo":{"status":"ok","timestamp":1660689287252,"user_tz":300,"elapsed":320,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Set random seed and create embedding layer (new embedding layer for each model)\n","tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n","                                     output_dim=128,\n","                                     embeddings_initializer=\"uniform\",\n","                                     input_length=max_length,\n","                                     name=\"embedding_5\")\n","\n","# Create 1-dimensional convolutional layer to model sequences\n","from tensorflow.keras import layers\n","inputs = layers.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorizer(inputs)\n","x = model_5_embedding(x)\n","x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\n","x = layers.GlobalMaxPool1D()(x)\n","# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","\n","#create model instance \n","model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")"],"execution_count":79,"outputs":[]},{"cell_type":"code","source":["# Compile Conv1D model\n","model_5.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"],"metadata":{"id":"VfLpJCyYyxb7","executionInfo":{"status":"ok","timestamp":1660689290279,"user_tz":300,"elapsed":173,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["# Get a summary of our 1D convolution model\n","model_5.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGx9km2GyyJy","executionInfo":{"status":"ok","timestamp":1660689291127,"user_tz":300,"elapsed":145,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"9239b841-8b85-42f3-8b7c-2f5078f3c468"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_5_Conv1D\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_10 (InputLayer)       [(None, 1)]               0         \n","                                                                 \n"," text_vectorization_1 (TextV  (None, 15)               0         \n"," ectorization)                                                   \n","                                                                 \n"," embedding_5 (Embedding)     (None, 15, 128)           1280000   \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 11, 32)            20512     \n","                                                                 \n"," global_max_pooling1d_1 (Glo  (None, 32)               0         \n"," balMaxPooling1D)                                                \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 1,300,545\n","Trainable params: 1,300,545\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"9fzlaKm1ZrMX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e99285a-6762-4420-b274-38cb26d4d8c7","executionInfo":{"status":"ok","timestamp":1660691178070,"user_tz":300,"elapsed":10454,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit the model\n","model_5_history = model_5.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n","                                                                     \"Conv1D\")])"],"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/Conv1D/20220816-230607\n","Epoch 1/5\n","215/215 [==============================] - 2s 10ms/step - loss: 0.0439 - accuracy: 0.9810 - val_loss: 0.8268 - val_accuracy: 0.7612\n","Epoch 2/5\n","215/215 [==============================] - 1s 7ms/step - loss: 0.0433 - accuracy: 0.9812 - val_loss: 0.8286 - val_accuracy: 0.7651\n","Epoch 3/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.0414 - accuracy: 0.9807 - val_loss: 0.8490 - val_accuracy: 0.7690\n","Epoch 4/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.0410 - accuracy: 0.9820 - val_loss: 0.8522 - val_accuracy: 0.7769\n","Epoch 5/5\n","215/215 [==============================] - 1s 6ms/step - loss: 0.0389 - accuracy: 0.9826 - val_loss: 0.8537 - val_accuracy: 0.7690\n"]}]},{"cell_type":"code","metadata":{"id":"ZHYw5GkxZ2OK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"15d4e4d8-a5d6-428e-8aae-bb6e08c9d718","executionInfo":{"status":"ok","timestamp":1660689359636,"user_tz":300,"elapsed":244,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions with model_5\n","model_5_pred_probs = model_5.predict(val_sentences)\n","model_5_pred_probs[:10]"],"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.22534485],\n","       [0.7534112 ],\n","       [0.9995602 ],\n","       [0.0556279 ],\n","       [0.01449848],\n","       [0.9858518 ],\n","       [0.98418933],\n","       [0.99758804],\n","       [0.99862623],\n","       [0.26914385]], dtype=float32)"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"v9YqTtjiaauS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f57dc32f-d308-49a9-ad45-0c636ec99f32","executionInfo":{"status":"ok","timestamp":1660689361748,"user_tz":300,"elapsed":171,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Convert model_5 prediction probabilities to labels\n","model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n","model_5_preds[:10]"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"wMY3s1Pnaj34","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c73b313d-8ac2-4499-9e1c-6c48cf7bed53","executionInfo":{"status":"ok","timestamp":1660689362769,"user_tz":300,"elapsed":116,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate model_5 evaluation metrics \n","model_5_results = calculate_results(y_true=val_labels, \n","                                    y_pred=model_5_preds)\n","model_5_results"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 77.82152230971128,\n"," 'precision': 78.07522349051432,\n"," 'recall': 77.82152230971128,\n"," 'f1': 77.58810170952619}"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","metadata":{"id":"wRfF4B6_at8k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b0292b0-a21a-4ef2-c251-fa5d242ba69d","executionInfo":{"status":"ok","timestamp":1660689364124,"user_tz":300,"elapsed":104,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compare model_5 results to baseline \n","compare_baseline_to_new_results(baseline_results, model_5_results)"],"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 77.82, Difference: -1.44\n","Baseline precision: 81.11, New precision: 78.08, Difference: -3.04\n","Baseline recall: 79.27, New recall: 77.82, Difference: -1.44\n","Baseline f1: 78.62, New f1: 77.59, Difference: -1.03\n"]}]},{"cell_type":"markdown","metadata":{"id":"R-NQ2MA5GZBo"},"source":["### Model 6: TensorFlow Hub Pretrained Sentence Encoder\n","\n","The main difference between the embedding layer we created and the Universal Sentence Encoder is that rather than create a word-level embedding, the Universal Sentence Encoder, as you might've guessed, creates a whole sentence-level embedding.\n","\n","Our embedding layer also outputs an a 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence.\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-USE-tensorflow-hub-encoder-decoder-model.png)\n","*The feature extractor model we're building through the eyes of an **encoder/decoder** model.*\n","\n","> 🔑 **Note:** An **encoder** is the name for a model which converts raw data such as text into a numerical representation (feature vector), a **decoder** converts the numerical representation to a desired output.\n","\n","As usual, this is best demonstrated with an example.\n","\n","We can load in a TensorFlow Hub module using the [`hub.load()`](https://www.tensorflow.org/hub/api_docs/python/hub/load) method and passing it the target URL of the module we'd like to use, in our case, it's \"https://tfhub.dev/google/universal-sentence-encoder/4\".\n","\n","Let's load the Universal Sentence Encoder (USE) model and test it on a couple of sentences."]},{"cell_type":"code","metadata":{"id":"7piW5jtxbUkV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c25f07d3-a5d3-4382-f7ee-90f0bf3246ee","executionInfo":{"status":"ok","timestamp":1660690900078,"user_tz":300,"elapsed":4937,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n","import tensorflow_hub as hub\n","USE_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n","embed = hub.load(use_url) # load USE model \n","embed_samples = embed([sample_sentence,\n","                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n","\n","print(embed_samples[0][:50])"],"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[-0.01157028  0.0248591   0.02878048 -0.012715    0.03971538  0.0882776\n","  0.02680984  0.05589836 -0.0106873  -0.00597291  0.00639323 -0.01819518\n","  0.00030813  0.09105888  0.05874644 -0.03180628  0.01512474 -0.05162929\n","  0.00991367 -0.06865347 -0.04209306  0.02678981  0.03011006  0.00321069\n"," -0.00337973 -0.04787357  0.0226672  -0.00985925 -0.04063613 -0.01292092\n"," -0.04666384  0.05630299 -0.03949255  0.00517686  0.02495829 -0.0701444\n","  0.02871508  0.04947684 -0.00633979 -0.08960192  0.02807118 -0.00808364\n"," -0.01360602  0.0599865  -0.10361787 -0.05195374  0.00232954 -0.02332531\n"," -0.03758105  0.03327728], shape=(50,), dtype=float32)\n"]}]},{"cell_type":"code","metadata":{"id":"vvArnKkGb4vu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"314004eb-8fee-4650-f25f-8982c6c80069","executionInfo":{"status":"ok","timestamp":1660690726598,"user_tz":300,"elapsed":181,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Each sentence has been encoded into a 512 dimension vector\n","embed_samples[0].shape"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([512])"]},"metadata":{},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"ZxYFDkGD-XjF"},"source":["Passing our sentences to the Universal Sentence Encoder (USE) encodes them from strings to 512 dimensional vectors, which make no sense to us but hopefully make sense to our machine learning models.\n","\n","Speaking of models, let's build one with the USE as our embedding layer.\n","\n","We can convert the TensorFlow Hub USE module into a Keras layer using the [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) class.\n","\n","> 🔑 **Note:** Due to the size of the USE TensorFlow Hub module, it may take a little while to download. Once it's downloaded though, it'll be cached and ready to use. And as with many TensorFlow Hub modules, there is a [\"lite\" version of the USE](https://tfhub.dev/google/universal-sentence-encoder-lite/2) which takes up less space but sacrifices some performance and requires more preprocessing steps. However, depending on your available compute power, the lite version may be better for your application use case."]},{"cell_type":"code","metadata":{"id":"ZcbBj0aXqrs9","executionInfo":{"status":"ok","timestamp":1660690921569,"user_tz":300,"elapsed":3962,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# We can use this encoding layer in place of our text_vectorizer and embedding layer\n","sentence_encoder_layer = hub.KerasLayer(USE_url,\n","                                        input_shape=[], # shape of inputs coming to our model \n","                                        dtype=tf.string, # data type of inputs coming to the USE layer\n","                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n","                                        name=\"USE\") "],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_pjIvPuYltA","executionInfo":{"status":"ok","timestamp":1660691308454,"user_tz":300,"elapsed":320,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Create model using the Sequential API\n","model_6 = tf.keras.Sequential([\n","  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n","  layers.Dense(64, activation=\"relu\"), #add extra layer \n","  layers.Dense(1, activation=\"sigmoid\")\n","], name=\"model_6_USE\")"],"execution_count":106,"outputs":[]},{"cell_type":"code","source":["# Compile model\n","model_6.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"],"metadata":{"id":"5iQmK9YT5UIA","executionInfo":{"status":"ok","timestamp":1660691309163,"user_tz":300,"elapsed":6,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["#model summary \n","model_6.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEiPVK0f5VW7","executionInfo":{"status":"ok","timestamp":1660691310188,"user_tz":300,"elapsed":140,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"e1ab16f3-8a9c-4ca9-d9aa-a3cb56db7272"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6_USE\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," USE (KerasLayer)            (None, 512)               256797824 \n","                                                                 \n"," dense_13 (Dense)            (None, 64)                32832     \n","                                                                 \n"," dense_14 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 256,830,721\n","Trainable params: 32,897\n","Non-trainable params: 256,797,824\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"uX9S0YvafybG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a236c2ce-744d-404c-bbf2-9bf4725052dc","executionInfo":{"status":"ok","timestamp":1660691334448,"user_tz":300,"elapsed":22024,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Train a classifier on top of pretrained embeddings\n","model_6_history = model_6.fit(train_sentences,\n","                              train_labels,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n","                                                                     \"tf_hub_sentence_encoder\")])"],"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20220816-230832\n","Epoch 1/5\n","215/215 [==============================] - 8s 26ms/step - loss: 0.5026 - accuracy: 0.7885 - val_loss: 0.4470 - val_accuracy: 0.7992\n","Epoch 2/5\n","215/215 [==============================] - 3s 14ms/step - loss: 0.4142 - accuracy: 0.8151 - val_loss: 0.4354 - val_accuracy: 0.8136\n","Epoch 3/5\n","215/215 [==============================] - 3s 14ms/step - loss: 0.3997 - accuracy: 0.8219 - val_loss: 0.4312 - val_accuracy: 0.8136\n","Epoch 4/5\n","215/215 [==============================] - 3s 14ms/step - loss: 0.3927 - accuracy: 0.8272 - val_loss: 0.4273 - val_accuracy: 0.8123\n","Epoch 5/5\n","215/215 [==============================] - 3s 14ms/step - loss: 0.3867 - accuracy: 0.8288 - val_loss: 0.4287 - val_accuracy: 0.8150\n"]}]},{"cell_type":"code","metadata":{"id":"xeyNXqU-gM2p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bdbb7a7f-13ad-4a6c-d1ee-5dd9b6b3874d","executionInfo":{"status":"ok","timestamp":1660691339964,"user_tz":300,"elapsed":742,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions with USE TF Hub model\n","model_6_pred_probs = model_6.predict(val_sentences)\n","model_6_pred_probs[:10]"],"execution_count":110,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.15846847],\n","       [0.7529948 ],\n","       [0.9896612 ],\n","       [0.21733744],\n","       [0.71393305],\n","       [0.6794042 ],\n","       [0.9824928 ],\n","       [0.9777313 ],\n","       [0.92804295],\n","       [0.08917005]], dtype=float32)"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"Gbn1Z0FfgVdx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cecb2684-bd56-49e8-8415-8da4bd81b073","executionInfo":{"status":"ok","timestamp":1660691340304,"user_tz":300,"elapsed":3,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Convert prediction probabilities to labels\n","model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n","model_6_preds[:10]"],"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"N2Ow2de3okcb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"937ca27a-468c-4fc4-ea92-9de97ea5f00d","executionInfo":{"status":"ok","timestamp":1660691341863,"user_tz":300,"elapsed":115,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate model 6 performance metrics\n","model_6_results = calculate_results(val_labels, model_6_preds)\n","model_6_results"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 81.49606299212599,\n"," 'precision': 81.79063565195398,\n"," 'recall': 81.49606299212599,\n"," 'f1': 81.32300658103408}"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"-BHnRHHHgp1r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"087118ae-3498-47f0-f579-8bb848fa1fae","executionInfo":{"status":"ok","timestamp":1660691343913,"user_tz":300,"elapsed":121,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compare TF Hub model to baseline\n","compare_baseline_to_new_results(baseline_results, model_6_results)"],"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 81.50, Difference: 2.23\n","Baseline precision: 81.11, New precision: 81.79, Difference: 0.68\n","Baseline recall: 79.27, New recall: 81.50, Difference: 2.23\n","Baseline f1: 78.62, New f1: 81.32, Difference: 2.70\n"]}]},{"cell_type":"markdown","metadata":{"id":"LHwu4QjijYWG"},"source":["### Model 7: TensorFlow Hub Pretrained Sentence Encoder 10% of the training data\n","\n","One of the benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract).\n","\n","To put this to the test, we're going to make a small subset of the training data (10%), train a model and evaluate it."]},{"cell_type":"code","metadata":{"id":"W5Sal8DpjzWm"},"source":["### NOTE: Making splits like this will lead to data leakage ###\n","### (some of the training examples in the validation set) ###\n","\n","### WRONG WAY TO MAKE SPLITS (train_df_shuffled has already been split) ### \n","\n","# # Create subsets of 10% of the training data\n","# train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n","# train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n","# train_labels_10_percent = train_10_percent[\"target\"].to_list()\n","# len(train_sentences_10_percent), len(train_labels_10_percent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHgowC3GUPJH","executionInfo":{"status":"ok","timestamp":1660691846586,"user_tz":300,"elapsed":147,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# One kind of correct way (there are more) to make data subset\n","# (split the already split train_sentences/train_labels)\n","train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),\n","                                                                                                                            train_labels,\n","                                                                                                                            test_size=0.1,\n","                                                                                                                            random_state=42)\n"],"execution_count":114,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8jaydmiVnJP","outputId":"7c753a6a-945e-4ae4-fccd-73e33d0cb644","executionInfo":{"status":"ok","timestamp":1660691848740,"user_tz":300,"elapsed":165,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check length of 10 percent datasets\n","print(f\"Total training examples: {len(train_sentences)}\")\n","print(f\"Length of 10% training examples: {len(train_sentences_10_percent)}\")"],"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training examples: 6851\n","Length of 10% training examples: 686\n"]}]},{"cell_type":"markdown","metadata":{"id":"7E2jr7rSEYT8"},"source":["Because we've selected a random subset of the training samples, the classes should be roughly balanced (as they are in the full training dataset)."]},{"cell_type":"code","metadata":{"id":"V0lEpFT0k0RB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"acf39efc-30f7-475b-8878-870ab93ff3d5","executionInfo":{"status":"ok","timestamp":1660692023736,"user_tz":300,"elapsed":120,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check the number of targets in our subset of data \n","# (this should be close to the distribution of labels in the original train_labels)\n","pd.Series(train_labels_10_percent).value_counts()"],"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    415\n","1    271\n","dtype: int64"]},"metadata":{},"execution_count":116}]},{"cell_type":"markdown","metadata":{"id":"ghl1qeGOEnXG"},"source":["To make sure we're making an appropriate comparison between our model's ability to learn from the full training set and 10% subset, we'll clone our USE model (`model_6`) using the [`tf.keras.models.clone_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model) method.\n","\n","Doing this will create the same architecture but reset the learned weights of the clone target (pretrained weights from the USE will remain but all others will be reset)."]},{"cell_type":"code","metadata":{"id":"PGmxeAOBjdg2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee9062a5-ac35-4481-c1a6-ef8ede7dade9","executionInfo":{"status":"ok","timestamp":1660692046367,"user_tz":300,"elapsed":3932,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Clone model_6 but reset weights\n","model_7 = tf.keras.models.clone_model(model_6)\n","\n","# Compile model\n","model_7.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","# Get a summary (will be same as model_6)\n","model_7.summary()"],"execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6_USE\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," USE (KerasLayer)            (None, 512)               256797824 \n","                                                                 \n"," dense_13 (Dense)            (None, 64)                32832     \n","                                                                 \n"," dense_14 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 256,830,721\n","Trainable params: 32,897\n","Non-trainable params: 256,797,824\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"LklU2maOkgUF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c4145b9-d177-4f7a-ca94-ac8a75334c0e","executionInfo":{"status":"ok","timestamp":1660692075198,"user_tz":300,"elapsed":7463,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Fit the model to 10% of the training data\n","model_7_history = model_7.fit(x=train_sentences_10_percent,\n","                              y=train_labels_10_percent,\n","                              epochs=5,\n","                              validation_data=(val_sentences, val_labels),\n","                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"10_percent_tf_hub_sentence_encoder\")])"],"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving TensorBoard log files to: model_logs/10_percent_tf_hub_sentence_encoder/20220816-232107\n","Epoch 1/5\n","22/22 [==============================] - 3s 45ms/step - loss: 0.6677 - accuracy: 0.6676 - val_loss: 0.6514 - val_accuracy: 0.6693\n","Epoch 2/5\n","22/22 [==============================] - 1s 28ms/step - loss: 0.6004 - accuracy: 0.7901 - val_loss: 0.5985 - val_accuracy: 0.7310\n","Epoch 3/5\n","22/22 [==============================] - 1s 25ms/step - loss: 0.5261 - accuracy: 0.8178 - val_loss: 0.5450 - val_accuracy: 0.7559\n","Epoch 4/5\n","22/22 [==============================] - 1s 29ms/step - loss: 0.4610 - accuracy: 0.8280 - val_loss: 0.5107 - val_accuracy: 0.7717\n","Epoch 5/5\n","22/22 [==============================] - 1s 29ms/step - loss: 0.4163 - accuracy: 0.8367 - val_loss: 0.4924 - val_accuracy: 0.7769\n"]}]},{"cell_type":"code","metadata":{"id":"ot6MRnznlgCL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ecbc81c2-70ed-44ae-bfe8-0036d21da614","executionInfo":{"status":"ok","timestamp":1660692080312,"user_tz":300,"elapsed":694,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make predictions with the model trained on 10% of the data\n","model_7_pred_probs = model_7.predict(val_sentences)\n","model_7_pred_probs[:10]"],"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.26267046],\n","       [0.7852249 ],\n","       [0.8816384 ],\n","       [0.28731787],\n","       [0.58576685],\n","       [0.82043076],\n","       [0.80411756],\n","       [0.8412294 ],\n","       [0.80572814],\n","       [0.13951766]], dtype=float32)"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"Vj_4aZellpRu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbd135e9-ea93-44be-da7a-819906e14602","executionInfo":{"status":"ok","timestamp":1660692082650,"user_tz":300,"elapsed":137,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Convert prediction probabilities to labels\n","model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n","model_7_preds[:10]"],"execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"T_lTXrDblyva","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db9b11e7-4512-4afb-ddb2-fad98c66224e","executionInfo":{"status":"ok","timestamp":1660692084705,"user_tz":300,"elapsed":4,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate model results\n","model_7_results = calculate_results(val_labels, model_7_preds)\n","model_7_results"],"execution_count":121,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 77.69028871391076,\n"," 'precision': 78.09693289921039,\n"," 'recall': 77.69028871391076,\n"," 'f1': 77.3916503042933}"]},"metadata":{},"execution_count":121}]},{"cell_type":"code","metadata":{"id":"G84ezltll6DT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7f401e9-4516-4887-8a02-4b9eb6b1c4d0","executionInfo":{"status":"ok","timestamp":1660692085832,"user_tz":300,"elapsed":137,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Compare to baseline\n","compare_baseline_to_new_results(baseline_results, model_7_results)"],"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline accuracy: 79.27, New accuracy: 77.69, Difference: -1.57\n","Baseline precision: 81.11, New precision: 78.10, Difference: -3.02\n","Baseline recall: 79.27, New recall: 77.69, Difference: -1.57\n","Baseline f1: 78.62, New f1: 77.39, Difference: -1.23\n"]}]},{"cell_type":"markdown","metadata":{"id":"iBs9V61EGh0J"},"source":["## Comparing the performance of each of our models\n","\n","* To visualize our model's performances, let's create a pandas DataFrame we our results dictionaries and then plot it."]},{"cell_type":"code","metadata":{"id":"Ex0NSaz7lRf-","colab":{"base_uri":"https://localhost:8080/","height":300},"outputId":"9c6a508f-04f7-4d5f-b9d9-71eeab8b9249","executionInfo":{"status":"ok","timestamp":1660692215422,"user_tz":300,"elapsed":184,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Combine model results into a DataFrame\n","all_model_results = pd.DataFrame({\"baseline\": baseline_results,\n","                                  \"simple_dense\": model_1_results,\n","                                  \"lstm\": model_2_results,\n","                                  \"gru\": model_3_results,\n","                                  \"bidirectional\": model_4_results,\n","                                  \"conv1d\": model_5_results,\n","                                  \"tf_hub_sentence_encoder\": model_6_results,\n","                                  \"tf_hub_10_percent_data\": model_7_results})\n","all_model_results = all_model_results.transpose()\n","all_model_results"],"execution_count":123,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                          accuracy  precision     recall         f1\n","baseline                 79.265092  81.113900  79.265092  78.621898\n","simple_dense             78.740157  79.149206  78.740157  78.469665\n","lstm                     75.065617  75.100780  75.065617  74.892686\n","gru                      76.771654  76.754509  76.771654  76.679327\n","bidirectional            76.640420  76.658954  76.640420  76.512135\n","conv1d                   77.821522  78.075223  77.821522  77.588102\n","tf_hub_sentence_encoder  81.496063  81.790636  81.496063  81.323007\n","tf_hub_10_percent_data   77.690289  78.096933  77.690289  77.391650"],"text/html":["\n","  <div id=\"df-c601df56-830f-41ae-9d4f-0a8a5a982a7f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline</th>\n","      <td>79.265092</td>\n","      <td>81.113900</td>\n","      <td>79.265092</td>\n","      <td>78.621898</td>\n","    </tr>\n","    <tr>\n","      <th>simple_dense</th>\n","      <td>78.740157</td>\n","      <td>79.149206</td>\n","      <td>78.740157</td>\n","      <td>78.469665</td>\n","    </tr>\n","    <tr>\n","      <th>lstm</th>\n","      <td>75.065617</td>\n","      <td>75.100780</td>\n","      <td>75.065617</td>\n","      <td>74.892686</td>\n","    </tr>\n","    <tr>\n","      <th>gru</th>\n","      <td>76.771654</td>\n","      <td>76.754509</td>\n","      <td>76.771654</td>\n","      <td>76.679327</td>\n","    </tr>\n","    <tr>\n","      <th>bidirectional</th>\n","      <td>76.640420</td>\n","      <td>76.658954</td>\n","      <td>76.640420</td>\n","      <td>76.512135</td>\n","    </tr>\n","    <tr>\n","      <th>conv1d</th>\n","      <td>77.821522</td>\n","      <td>78.075223</td>\n","      <td>77.821522</td>\n","      <td>77.588102</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_sentence_encoder</th>\n","      <td>81.496063</td>\n","      <td>81.790636</td>\n","      <td>81.496063</td>\n","      <td>81.323007</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_10_percent_data</th>\n","      <td>77.690289</td>\n","      <td>78.096933</td>\n","      <td>77.690289</td>\n","      <td>77.391650</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c601df56-830f-41ae-9d4f-0a8a5a982a7f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c601df56-830f-41ae-9d4f-0a8a5a982a7f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c601df56-830f-41ae-9d4f-0a8a5a982a7f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"v-s2DSLpmM1F","executionInfo":{"status":"ok","timestamp":1660692240171,"user_tz":300,"elapsed":115,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Reduce the accuracy to same scale as other metrics\n","all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100"],"execution_count":124,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wp69bR8umD5g","colab":{"base_uri":"https://localhost:8080/","height":546},"outputId":"2adcf10d-0b72-46ca-a3e0-2acd37d6f5bf","executionInfo":{"status":"ok","timestamp":1660692289942,"user_tz":300,"elapsed":622,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Plot and compare all of the model results\n","all_model_results[\"accuracy\"].plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));"],"execution_count":126,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Plot and compare all of the model results\n","all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"id":"sww8sBdx-ZOM","executionInfo":{"status":"ok","timestamp":1660692313691,"user_tz":300,"elapsed":695,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"530cb999-8039-4fd1-94c5-e5158b5e7ce1"},"execution_count":127,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"avbdkiIuKNNr"},"source":["Looks like our pretrained USE TensorFlow Hub models have the best performance, even the one with only 10% of the training data seems to outperform the other models. This goes to show the power of transfer learning.\n","\n","How about we drill down and get the F1-score's of each model?"]},{"cell_type":"code","metadata":{"id":"yktdOiufmm3p","colab":{"base_uri":"https://localhost:8080/","height":546},"outputId":"3ff3cdf9-991a-489f-c3d6-30a36f4ff1a6","executionInfo":{"status":"ok","timestamp":1660692366061,"user_tz":300,"elapsed":388,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Sort model results by f1-score\n","all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7), color=\"teal\");"],"execution_count":128,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"pv2iE0TPGdNy"},"source":["Drilling down into a single metric we see our USE TensorFlow Hub models performing  better than all of the other models. Interestingly, the baseline's F1-score isn't too far off the rest of the deeper models.\n","\n","We can also visualize all of our model's training logs using TensorBoard.dev."]},{"cell_type":"code","metadata":{"id":"2Ca8TalwGhPf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660692699608,"user_tz":300,"elapsed":18303,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}},"outputId":"96b38f2e-14be-4586-a946-1975909ebd08"},"source":["# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n","# # Upload TensorBoard dev records\n","!tensorboard dev upload --logdir /content/model_logs \\\n","   --name \"NLP modelling experiments\" \\\n","   --description \"A series of different NLP modellings experiments with various models\" \\\n","   --one_shot # exits the uploader when upload has finished"],"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/bwLoxad8RGWX7055RQYWWA/\n","\n","\u001b[1m[2022-08-16T23:31:28]\u001b[0m Started scanning logdir.\n","\u001b[1m[2022-08-16T23:31:38]\u001b[0m Total uploaded: 360 scalars, 0 tensors, 11 binary objects (4.6 MB)\n","\u001b[1m[2022-08-16T23:31:38]\u001b[0m Done scanning logdir.\n","\n","\n","Done. View your TensorBoard at https://tensorboard.dev/experiment/bwLoxad8RGWX7055RQYWWA/\n"]}]},{"cell_type":"code","metadata":{"id":"Os7dv00u21jg"},"source":["# If you need to remove previous experiments, you can do so using the following command\n","# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGVZhTTiGdd5"},"source":["## Combining our models (model ensembling/stacking)\n","\n","Many production systems use an **ensemble** (multiple different models combined) of models to make a prediction.\n","\n","The idea behind model stacking is that if several uncorrelated models agree on a prediction, then the prediction must be more robust than a prediction made by a singular model.\n","\n","The keyword in the sentence above is **uncorrelated**, which is another way of saying, different types of models. For example, in our case, we might combine our baseline, our bidirectional model and our TensorFlow Hub USE model.\n","\n","Although these models are all trained on the same data, they all have a different way of finding patterns.\n","\n","If we were to use three similarly trained models, such as three LSTM models, the predictions they output will likely be very similar.\n","\n","Think of it as trying to decide where to eat with your friends. If you all have similar tastes, you'll probably all pick the same restaurant. But if you've all got different tastes and still end up picking the same restaurant, the restaurant must be good.\n","\n","Since we're working with a classification problem, there are a few of ways we can combine our models:\n","1. **Averaging** - Take the output prediction probabilities of each model for each sample, combine them and then average them.\n","2. **Majority vote (mode)** - Make class predictions with each of your models on all samples, the predicted class is the one in majority. For example, if three different models predict `[1, 0, 1]` respectively, the majority class is `1`, therefore, that would be the predicted label.\n","3. **Model stacking** - Take the outputs of each of your chosen models and use them as inputs to another model.\n","\n","> 📖 **Resource:** The above methods for model stacking/ensembling were adapted from Chapter 6 of the [Machine Learning Engineering Book](http://www.mlebook.com/wiki/doku.php) by Andriy Burkov. If you're looking to enter the field of machine learning engineering, not only building models but production-scale machine learning systems, I'd highly recommend reading it in its entirety.\n","\n","Again, the concept of model stacking is best seen in action.\n","\n","We're going to combine our baseline model (`model_0`), LSTM model (`model_2`) and our USE model trained on the full training data (`model_6`) by averaging the combined prediction probabilities of each."]},{"cell_type":"code","metadata":{"id":"t63u8PCCm-yo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"794094ce-7c38-4aad-802b-6f840c208378","executionInfo":{"status":"ok","timestamp":1660692970808,"user_tz":300,"elapsed":164,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Get mean pred probs for 3 models\n","baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model\n","combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs)\n","combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes\n","combined_preds[:20]"],"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n","       0., 0., 1.], dtype=float32)>"]},"metadata":{},"execution_count":132}]},{"cell_type":"markdown","metadata":{"id":"6abZa7wqlXSI"},"source":["Wonderful! We've got a combined predictions array of different classes, let's evaluate them against the true labels and add our stacked model's results to our `all_model_results` DataFrame."]},{"cell_type":"code","metadata":{"id":"ieYvhDiev8Et","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c921294-f6b8-4bc3-86b8-ab0201f4bf22","executionInfo":{"status":"ok","timestamp":1660693000215,"user_tz":300,"elapsed":168,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate results from averaging the prediction probabilities\n","ensemble_results = calculate_results(val_labels, combined_preds)\n","ensemble_results"],"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 77.69028871391076,\n"," 'precision': 77.65615284994071,\n"," 'recall': 77.69028871391076,\n"," 'f1': 77.64677606660332}"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","metadata":{"id":"132EHlUUpRrP","executionInfo":{"status":"ok","timestamp":1660693003251,"user_tz":300,"elapsed":139,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Add our combined model's results to the results DataFrame\n","all_model_results.loc[\"ensemble_results\"] = ensemble_results"],"execution_count":134,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pm2P1zsvpZ3D","executionInfo":{"status":"ok","timestamp":1660693004089,"user_tz":300,"elapsed":135,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Convert the accuracy to the same scale as the rest of the results\n","all_model_results.loc[\"ensemble_results\"][\"accuracy\"] = all_model_results.loc[\"ensemble_results\"][\"accuracy\"]/100"],"execution_count":135,"outputs":[]},{"cell_type":"code","metadata":{"id":"trmdZ6eEpwHI","colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"9fedfcaf-7a99-42ef-cdf8-81f3ff451774","executionInfo":{"status":"ok","timestamp":1660693004973,"user_tz":300,"elapsed":123,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["all_model_results"],"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                         accuracy  precision     recall         f1\n","baseline                 0.792651  81.113900  79.265092  78.621898\n","simple_dense             0.787402  79.149206  78.740157  78.469665\n","lstm                     0.750656  75.100780  75.065617  74.892686\n","gru                      0.767717  76.754509  76.771654  76.679327\n","bidirectional            0.766404  76.658954  76.640420  76.512135\n","conv1d                   0.778215  78.075223  77.821522  77.588102\n","tf_hub_sentence_encoder  0.814961  81.790636  81.496063  81.323007\n","tf_hub_10_percent_data   0.776903  78.096933  77.690289  77.391650\n","ensemble_results         0.776903  77.656153  77.690289  77.646776"],"text/html":["\n","  <div id=\"df-a6330952-a8aa-4bce-bcef-aed6701a390a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline</th>\n","      <td>0.792651</td>\n","      <td>81.113900</td>\n","      <td>79.265092</td>\n","      <td>78.621898</td>\n","    </tr>\n","    <tr>\n","      <th>simple_dense</th>\n","      <td>0.787402</td>\n","      <td>79.149206</td>\n","      <td>78.740157</td>\n","      <td>78.469665</td>\n","    </tr>\n","    <tr>\n","      <th>lstm</th>\n","      <td>0.750656</td>\n","      <td>75.100780</td>\n","      <td>75.065617</td>\n","      <td>74.892686</td>\n","    </tr>\n","    <tr>\n","      <th>gru</th>\n","      <td>0.767717</td>\n","      <td>76.754509</td>\n","      <td>76.771654</td>\n","      <td>76.679327</td>\n","    </tr>\n","    <tr>\n","      <th>bidirectional</th>\n","      <td>0.766404</td>\n","      <td>76.658954</td>\n","      <td>76.640420</td>\n","      <td>76.512135</td>\n","    </tr>\n","    <tr>\n","      <th>conv1d</th>\n","      <td>0.778215</td>\n","      <td>78.075223</td>\n","      <td>77.821522</td>\n","      <td>77.588102</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_sentence_encoder</th>\n","      <td>0.814961</td>\n","      <td>81.790636</td>\n","      <td>81.496063</td>\n","      <td>81.323007</td>\n","    </tr>\n","    <tr>\n","      <th>tf_hub_10_percent_data</th>\n","      <td>0.776903</td>\n","      <td>78.096933</td>\n","      <td>77.690289</td>\n","      <td>77.391650</td>\n","    </tr>\n","    <tr>\n","      <th>ensemble_results</th>\n","      <td>0.776903</td>\n","      <td>77.656153</td>\n","      <td>77.690289</td>\n","      <td>77.646776</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6330952-a8aa-4bce-bcef-aed6701a390a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a6330952-a8aa-4bce-bcef-aed6701a390a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a6330952-a8aa-4bce-bcef-aed6701a390a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":136}]},{"cell_type":"markdown","metadata":{"id":"HZwqwF_swdIA"},"source":["How did the stacked model go against the other models?\n","\n","> 🔑 **Note:** It seems many of our model's results are similar. This may mean there are some limitations to what can be learned from our data. When many of your modelling experiments return similar results, it's a good idea to revisit your data, we'll do this shortly."]},{"cell_type":"markdown","metadata":{"id":"UpwErZOgX_nC"},"source":["## Saving and loading a trained model\n","\n","Although training time didn't take very long, it's good practice to save your trained models to avoid having to retrain them.\n","\n","Saving your models also enables you to export them for use elsewhere outside of your notebooks, such as in a web application.\n","\n","There are two main ways of [saving a model in TensorFlow](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model):\n","1. The `HDF5` format. \n","2. The `SavedModel` format (default).\n","\n","Let's take a look at both."]},{"cell_type":"code","metadata":{"id":"SlwjGFVyX-_T"},"source":["# Save TF Hub Sentence Encoder model to HDF5 format\n","model_6.save(\"model_6.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cp6zvmprm9A3"},"source":["If you save a model as a `HDF5`, when loading it back in, you need to let [TensorFlow know about any custom objects you've used](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects) (e.g. components which aren't built from pure TensorFlow, such as TensorFlow Hub components)."]},{"cell_type":"code","metadata":{"id":"sSINZ0Q-nRb2"},"source":["# Load model with custom Hub Layer (required with HDF5 format)\n","loaded_model_6 = tf.keras.models.load_model(\"model_6.h5\", \n","                                            custom_objects={\"KerasLayer\": hub.KerasLayer})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4BCJ8iXnZ4r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e53af7df-411d-4a91-f1f3-194d4dd16891"},"source":["# How does our loaded model perform?\n","loaded_model_6.evaluate(val_sentences, val_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 1s 9ms/step - loss: 0.4309 - accuracy: 0.8123\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.43088313937187195, 0.8123359680175781]"]},"metadata":{},"execution_count":112}]},{"cell_type":"markdown","metadata":{"id":"02rbT4fwn0It"},"source":["Calling the `save()` method on our target model and passing it a filepath allows us to save our model in the `SavedModel` format. "]},{"cell_type":"code","metadata":{"id":"e3eVaNBDoMsv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8800d005-ac0c-40e0-8d58-9bc220dd1989"},"source":["# Save TF Hub Sentence Encoder model to SavedModel format (default)\n","model_6.save(\"model_6_SavedModel_format\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) USE_input with unsupported characters which will be renamed to use_input in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: model_6_SavedModel_format/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: model_6_SavedModel_format/assets\n"]}]},{"cell_type":"markdown","metadata":{"id":"l-t01S-JoOqK"},"source":["If you use SavedModel format (default), you can reload your model without specifying custom objects using the [`tensorflow.keras.models.load_model()`](https://www.tensorflow.org/tutorials/keras/save_and_load) function."]},{"cell_type":"code","metadata":{"id":"Dw3zf4fVoU5H"},"source":["# Load TF Hub Sentence Encoder SavedModel\n","loaded_model_6_SavedModel = tf.keras.models.load_model(\"model_6_SavedModel_format\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqiPr6iiofi1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed6ece1e-a8d1-4622-e24c-4841d1e2879f"},"source":["# Evaluate loaded SavedModel format\n","loaded_model_6_SavedModel.evaluate(val_sentences, val_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 1s 11ms/step - loss: 0.4309 - accuracy: 0.8123\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.43088313937187195, 0.8123359680175781]"]},"metadata":{},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"xzp3SHi3oQ3u"},"source":["As you can see saving and loading our model with either format results in the same performance.\n","\n","> 🤔 **Question:** Should you used the `SavedModel` format or `HDF5` format?\n","\n","For most use cases, the `SavedModel` format will suffice. However, this is a TensorFlow specific standard. If you need a more general-purpose data standard, `HDF5` might be better. For more, check out the [TensorFlow documentation on saving and loading models](https://www.tensorflow.org/tutorials/keras/save_and_load)."]},{"cell_type":"markdown","metadata":{"id":"V5a1648rG3z1"},"source":["## Finding the most wrong examples\n","\n","We mentioned before that if many of our modelling experiments are returning similar results, despite using different kinds of models, it's a good idea to return to the data and inspect why this might be.\n","\n","One of the best ways to inspect your data is to sort your model's predictions and find the samples it got *most* wrong, meaning, what predictions had a high prediction probability but turned out to be wrong.\n","\n","Once again, visualization is your friend. Visualize, visualize, visualize.\n","\n","To make things visual, let's take our best performing model's prediction probabilities and classes along with the validation samples (text and ground truth labels) and combine them in a pandas DataFrame.\n","\n","* If our best model still isn't perfect, what examples is it getting wrong? \n","* Which ones are the *most* wrong?\n","* Are there some labels which are wrong? E.g. the model gets it right but the ground truth label doesn't reflect this"]},{"cell_type":"code","metadata":{"id":"gnHfX--TwMIW","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"9d33aa7c-0e00-4e8c-d794-1903e8f8f5df","executionInfo":{"status":"ok","timestamp":1660693348515,"user_tz":300,"elapsed":154,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Create dataframe with validation sentences and best performing model predictions\n","val_df = pd.DataFrame({\"text\": val_sentences,\n","                       \"target\": val_labels,\n","                       \"pred\": model_6_preds,\n","                       \"pred_prob\": tf.squeeze(model_6_pred_probs)})\n","val_df.head()"],"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  target  pred  pred_prob\n","0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0   0.0   0.158468\n","1  FedEx no longer to transport bioterror germs i...       0   1.0   0.752995\n","2  Gunmen kill four in El Salvador bus attack: Su...       1   1.0   0.989661\n","3  @camilacabello97 Internally and externally scr...       1   0.0   0.217337\n","4  Radiation emergency #preparedness starts with ...       1   1.0   0.713933"],"text/html":["\n","  <div id=\"df-4ff71396-ef75-4bc5-8ea6-1377999e4113\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>pred</th>\n","      <th>pred_prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.158468</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FedEx no longer to transport bioterror germs i...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.752995</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.989661</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@camilacabello97 Internally and externally scr...</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.217337</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Radiation emergency #preparedness starts with ...</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.713933</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ff71396-ef75-4bc5-8ea6-1377999e4113')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4ff71396-ef75-4bc5-8ea6-1377999e4113 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4ff71396-ef75-4bc5-8ea6-1377999e4113');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":137}]},{"cell_type":"markdown","metadata":{"id":"SKJ9dTbPrIG4"},"source":["Now let's find our model's wrong predictions (where `target != pred`) and sort them by their prediction probability (the `pred_prob` column)."]},{"cell_type":"code","metadata":{"id":"0DwBXQS1wvZx","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"c3648725-9e0c-4045-b536-8643e0dd58c5","executionInfo":{"status":"ok","timestamp":1660693379197,"user_tz":300,"elapsed":119,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Find the wrong predictions and sort by prediction probabilities\n","most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False)\n","most_wrong[:10]"],"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  text  target  pred  \\\n","31   ? High Skies - Burning Buildings ? http://t.co...       0   1.0   \n","628  @noah_anyname That's where the concentration c...       0   1.0   \n","759  FedEx will no longer transport bioterror patho...       0   1.0   \n","49   @madonnamking RSPCA site multiple 7 story high...       0   1.0   \n","393  @SonofLiberty357 all illuminated by the bright...       0   1.0   \n","109  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...       0   1.0   \n","209  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0   1.0   \n","144                                 The Sound of Arson       0   1.0   \n","251  @AshGhebranious civil rights continued in the ...       0   1.0   \n","698  åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...       0   1.0   \n","\n","     pred_prob  \n","31    0.912037  \n","628   0.870788  \n","759   0.865981  \n","49    0.842742  \n","393   0.840571  \n","109   0.801912  \n","209   0.800398  \n","144   0.783631  \n","251   0.783409  \n","698   0.772568  "],"text/html":["\n","  <div id=\"df-88f48ebd-bfcd-475b-b3dd-5ba4895f6d61\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>pred</th>\n","      <th>pred_prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31</th>\n","      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.912037</td>\n","    </tr>\n","    <tr>\n","      <th>628</th>\n","      <td>@noah_anyname That's where the concentration c...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.870788</td>\n","    </tr>\n","    <tr>\n","      <th>759</th>\n","      <td>FedEx will no longer transport bioterror patho...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.865981</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>@madonnamking RSPCA site multiple 7 story high...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.842742</td>\n","    </tr>\n","    <tr>\n","      <th>393</th>\n","      <td>@SonofLiberty357 all illuminated by the bright...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.840571</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.801912</td>\n","    </tr>\n","    <tr>\n","      <th>209</th>\n","      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.800398</td>\n","    </tr>\n","    <tr>\n","      <th>144</th>\n","      <td>The Sound of Arson</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.783631</td>\n","    </tr>\n","    <tr>\n","      <th>251</th>\n","      <td>@AshGhebranious civil rights continued in the ...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.783409</td>\n","    </tr>\n","    <tr>\n","      <th>698</th>\n","      <td>åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.772568</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88f48ebd-bfcd-475b-b3dd-5ba4895f6d61')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-88f48ebd-bfcd-475b-b3dd-5ba4895f6d61 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-88f48ebd-bfcd-475b-b3dd-5ba4895f6d61');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":138}]},{"cell_type":"markdown","metadata":{"id":"r3VcRHOusB2D"},"source":["Finally, we can write some code to visualize the sample text, truth label, prediction class and prediction probability. Because we've sorted our samples by prediction probability, viewing samples from the head of our `most_wrong` DataFrame will show us false positives.\n","\n","A reminder:\n","* `0` = Not a real diaster Tweet\n","* `1` = Real diaster Tweet"]},{"cell_type":"code","metadata":{"id":"xLFYDEsoxRFP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e33dd22-890b-4791-cde8-8074deffd524","executionInfo":{"status":"ok","timestamp":1660693453498,"user_tz":300,"elapsed":165,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check the false positives (model predicted 1 when should've been 0)\n","for row in most_wrong[:10].itertuples(): # loop through the top 10 rows (change the index to view different rows)\n","  _, text, target, pred, prob = row\n","  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"----\\n\")"],"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 0, Pred: 1, Prob: 0.912037193775177\n","Text:\n","? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8707882761955261\n","Text:\n","@noah_anyname That's where the concentration camps and mass murder come in. \n"," \n","EVERY. FUCKING. TIME.\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8659806251525879\n","Text:\n","FedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8427421450614929\n","Text:\n","@madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8405709266662598\n","Text:\n","@SonofLiberty357 all illuminated by the brightly burning buildings all around the town!\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8019120693206787\n","Text:\n","[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.8003976345062256\n","Text:\n","Ashes 2015: AustraliaÛªs collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.7836310863494873\n","Text:\n","The Sound of Arson\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.7834089994430542\n","Text:\n","@AshGhebranious civil rights continued in the 60s. And what about trans-generational trauma? if anything we should listen to the Americans.\n","\n","----\n","\n","Target: 0, Pred: 1, Prob: 0.7725678086280823\n","Text:\n","åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Tent Collapse Story: Correction: Tent Collapse story åÈ http://t.co/fDJUYvZMrv @wizkidayo\n","\n","----\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"aXCH9J-UspWg"},"source":["We can view the bottom end of our `most_wrong` DataFrame to inspect false negatives (model predicts 0, not a real diaster Tweet, when it should've predicted 1, real diaster Tweet)."]},{"cell_type":"code","metadata":{"id":"6EaMchehxwLq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"127b7733-9fb1-42a9-eb2d-7188b6643b57","executionInfo":{"status":"ok","timestamp":1660693882323,"user_tz":300,"elapsed":176,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Check the most wrong false negatives (model predicted 0 when should've predict 1)\n","for row in most_wrong[-10:].itertuples():\n","  _, text, target, pred, prob = row\n","  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"----\\n\")"],"execution_count":140,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 1, Pred: 0, Prob: 0.06370054930448532\n","Text:\n","going to redo my nails and watch behind the scenes of desolation of smaug ayyy\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.06268582493066788\n","Text:\n","@BoyInAHorsemask its a panda trapped in a dogs body\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.06055450439453125\n","Text:\n","Lucas Duda is Ghost Rider. Not the Nic Cage version but an actual 'engulfed in flames' badass. #Mets\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.056094516068696976\n","Text:\n","You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.05279713496565819\n","Text:\n","@willienelson We need help! Horses will die!Please RT &amp; sign petition!Take a stand &amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.05046424642205238\n","Text:\n","I get to smoke my shit in peace\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.043516527861356735\n","Text:\n","Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.04268999025225639\n","Text:\n","@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.0401497520506382\n","Text:\n","Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n","\n","----\n","\n","Target: 1, Pred: 0, Prob: 0.039744727313518524\n","Text:\n","Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n","\n","----\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"lRKQPEAgtpJq"},"source":["Do you notice anything interesting about the most wrong samples?\n","\n","Are the ground truth labels correct? What do you think would happen if we went back and corrected the labels which aren't?"]},{"cell_type":"markdown","metadata":{"id":"U0W3DWgWJCWs"},"source":["## Making predictions on the test dataset\n","\n","Alright we've seen how our model's perform on the validation set.\n","\n","But how about the test dataset?\n","\n","We don't have labels for the test dataset so we're going to have to make some predictions and inspect them for ourselves.\n","\n","Let's write some code to make predictions on random samples from the test dataset and visualize them."]},{"cell_type":"code","metadata":{"id":"6Q9lgqoDyequ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bdce7607-dc8a-44a5-d470-ec555459eb5b","executionInfo":{"status":"ok","timestamp":1660694012841,"user_tz":300,"elapsed":865,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Making predictions on the test dataset\n","test_sentences = test_df[\"text\"].to_list()\n","test_samples = random.sample(test_sentences, 10)\n","for test_sample in test_samples:\n","  pred_prob = tf.squeeze(model_6.predict([test_sample])) # has to be list\n","  pred = tf.round(pred_prob)\n","  print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n","  print(f\"Text:\\n{test_sample}\\n\")\n","  print(\"----\\n\")"],"execution_count":141,"outputs":[{"output_type":"stream","name":"stdout","text":["Pred: 0, Prob: 0.23688679933547974\n","Text:\n","THIS SOUNDS LIKE A SONG YOU WOULD HEAR IN A MOVIE WHERE THEY ARE WALKING AWAY FROM BURNING BUILDINGS AND CARS AND SHIT\n","\n","----\n","\n","Pred: 1, Prob: 0.9900642037391663\n","Text:\n","Suspected Salvadoran gang members killed four people and wounded seven more on Wednesday in an attack on a bus as it travelled down a ruralÛ_\n","\n","----\n","\n","Pred: 1, Prob: 0.8070700764656067\n","Text:\n","RT RTphotographyUK: New #photo Oak in a snowstorm http://t.co/gWOKZELq5Q taken in #winter on the #SouthDowns #Hampshire #photography #artÛ_\n","\n","----\n","\n","Pred: 1, Prob: 0.9576754570007324\n","Text:\n","Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/6oPYSf87K2\n","\n","----\n","\n","Pred: 0, Prob: 0.3482828438282013\n","Text:\n","@kdudakia I get that there has been a massive turn away from Labour but that does not = electoral landslide. Credits tories too much and...\n","\n","----\n","\n","Pred: 0, Prob: 0.21423524618148804\n","Text:\n","According to the tabloids Cilla could've been saved. In the sense that Titanic wouldn't have sunk had it avoided the iceberg.\n","\n","----\n","\n","Pred: 0, Prob: 0.02487284317612648\n","Text:\n","I love dat lady '@Crhedrys: ???? you nko?'@Foxy__Siren: Oh finally Jennifer Aniston got married??????... I'm so happy for her ??????''\n","\n","----\n","\n","Pred: 0, Prob: 0.2448672652244568\n","Text:\n","When you say call dad but your phone dials emergency services..... #awesomejobsiri\n","\n","----\n","\n","Pred: 0, Prob: 0.23323436081409454\n","Text:\n","@kaytlinmartinez dude those enormous blizzards we destroyed from DQ ??\n","\n","----\n","\n","Pred: 1, Prob: 0.9148995876312256\n","Text:\n","Drove past the field on fire yesterday. Reminded me of the bush fires we used to get in South Africa. http://t.co/SN3dHuYnFe\n","\n","----\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"eT1jhk8xdod5"},"source":["## Predicting on Tweets from the wild\n","\n","How about we find some Tweets and use our model to predict whether or not they're about a diaster or not?\n","\n","To start, let's take one of my own [Tweets on living life like an ensemble model](https://twitter.com/mrdbourke/status/1313649328351662082). "]},{"cell_type":"code","metadata":{"id":"qHmXxuPH0aUB","executionInfo":{"status":"ok","timestamp":1660694178651,"user_tz":300,"elapsed":125,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Turn Tweet into string\n","daniels_tweet = \"Life like an ensemble: take the best choices from others and make your own\""],"execution_count":142,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPbZaGznvbEx"},"source":["Now we'll write a small function to take a model and an example sentence and return a prediction."]},{"cell_type":"code","metadata":{"id":"KyH9tn9upjld","executionInfo":{"status":"ok","timestamp":1660694242561,"user_tz":300,"elapsed":175,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["def predict_on_sentence(model, sentence):\n","  \"\"\"\n","  Uses model to make a prediction on sentence.\n","\n","  Returns the sentence, the predicted label and the prediction probability.\n","  \"\"\"\n","  pred_prob = model.predict([sentence])\n","  pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n","  print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label > 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")\n","  print(f\"Text:\\n{sentence}\")"],"execution_count":143,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxONpJV8qmWP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"71eb08b7-0de3-49b6-814e-8f5839173654","executionInfo":{"status":"ok","timestamp":1660694283670,"user_tz":300,"elapsed":156,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Make a prediction on Tweet from the wild\n","predict_on_sentence(model=model_6, # use the USE model\n","                    sentence=daniels_tweet)"],"execution_count":144,"outputs":[{"output_type":"stream","name":"stdout","text":["Pred: 0.0 (not real disaster) Prob: 0.05923913046717644\n","Text:\n","Life like an ensemble: take the best choices from others and make your own\n"]}]},{"cell_type":"markdown","metadata":{"id":"tYOfNacw08Of"},"source":["* Test on tweets regarding real disasters"]},{"cell_type":"code","metadata":{"id":"AqILBsTK2i9R","executionInfo":{"status":"ok","timestamp":1660694323360,"user_tz":300,"elapsed":2,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Source - https://twitter.com/BeirutCityGuide/status/1290696551376007168\n","beirut_tweet_1 = \"Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\"\n","\n","# Source - https://twitter.com/BeirutCityGuide/status/1290773498743476224\n","beirut_tweet_2 = \"#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon\""],"execution_count":145,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvlbHDISrVmX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2df4003c-b8d8-4536-e133-8903d601d263","executionInfo":{"status":"ok","timestamp":1660694327631,"user_tz":300,"elapsed":131,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Predict on diaster Tweet 1\n","predict_on_sentence(model=model_6, \n","                    sentence=beirut_tweet_1)"],"execution_count":146,"outputs":[{"output_type":"stream","name":"stdout","text":["Pred: 1.0 (real disaster) Prob: 0.9644551277160645\n","Text:\n","Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\n"]}]},{"cell_type":"code","metadata":{"id":"5uKYx11p2zCd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2a1534e-42f7-451c-a086-0643202a3a8d","executionInfo":{"status":"ok","timestamp":1660694331202,"user_tz":300,"elapsed":108,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Predict on diaster Tweet 2\n","predict_on_sentence(model=model_6, \n","                    sentence=beirut_tweet_2)"],"execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["Pred: 1.0 (real disaster) Prob: 0.9711828231811523\n","Text:\n","#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon\n"]}]},{"cell_type":"markdown","metadata":{"id":"fczP1dFcwe98"},"source":["Looks like our model is performing as expected, predicting both of the diaster Tweets as actual diasters.\n","\n","> 🔑 **Note:** The above examples are cherry-picked and are cases where you'd expect a model to function at high performance. For actual production systems, you'll want to continaully perform tests to see how your model is performing."]},{"cell_type":"markdown","metadata":{"id":"Fp0fkK-tHPRE"},"source":["## The speed/score tradeoff\n","\n","One of the final tests we're going to do is to find the speed/score tradeoffs between our best model and baseline model.\n","\n","Why is this important?\n","\n","Although it can be tempting to just choose the best performing model you find through experimentation, this model might not actually work in a production setting.\n","\n","Put it this way, imagine you're Twitter and receive 1 million Tweets per hour (this is a made up number, the actual number is much higher). And you're trying to build a diaster detection system to read Tweets and alert authorities with details about a diaster in close to real-time.\n","\n","Compute power isn't free so you're limited to a single compute machine for the project. On that machine, one of your models makes 10,000 predictions per second at 80% accuracy where as another one of your models (a larger model) makes 100 predictions per second at 85% accuracy.\n","\n","Which model do you choose?\n","\n","Is the second model's performance boost worth missing out on the extra capacity?\n","\n","Of course, there are many options you could try here, such as sending as many Tweets as possible to the first model and then sending the ones which the model is least certain of to the second model. \n","\n","The point here is to illustrate the best model you find through experimentation, might not be the model you end up using in production.\n","\n","To make this more concrete, let's write a function to take a model and a number of samples and time how long the given model takes to make predictions on those samples."]},{"cell_type":"code","metadata":{"id":"DnXp8DKOp3J6","executionInfo":{"status":"ok","timestamp":1660694882777,"user_tz":300,"elapsed":176,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate the time of predictions\n","import time\n","def pred_timer(model, samples):\n","  \"\"\"\n","  Times how long a model takes to make predictions on samples.\n","  \n","  Args:\n","  ----\n","  model = a trained model\n","  sample = a list of samples\n","\n","  Returns:\n","  ----\n","  total_time = total elapsed time for model to make predictions on samples\n","  time_per_pred = time in seconds per single sample\n","  \"\"\"\n","  start_time = time.perf_counter() # get start time\n","  model.predict(samples) # make predictions\n","  end_time = time.perf_counter() # get finish time\n","  total_time = end_time-start_time # calculate how long predictions took to make\n","  time_per_pred = total_time/len(val_sentences) # find prediction time per sample\n","  return total_time, time_per_pred"],"execution_count":148,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxWwS73hze6Z"},"source":["Let's use our `pred_timer()` function to evaluate the prediction times of our best performing model (`model_6`) and our baseline model (`model_0`)."]},{"cell_type":"code","metadata":{"id":"JMbGMIWd5c9N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad941630-5dce-4cd5-a5dc-83ae6a509c83","executionInfo":{"status":"ok","timestamp":1660694901752,"user_tz":300,"elapsed":458,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate TF Hub Sentence Encoder prediction times\n","model_6_total_pred_time, model_6_time_per_pred = pred_timer(model_6, val_sentences)\n","model_6_total_pred_time, model_6_time_per_pred"],"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.28502131800087227, 0.00037404372441059353)"]},"metadata":{},"execution_count":149}]},{"cell_type":"code","metadata":{"id":"I4ej2VyT5oQs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c61de9c-3fb9-4069-9371-a4666f2f34ce","executionInfo":{"status":"ok","timestamp":1660694903007,"user_tz":300,"elapsed":128,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["# Calculate Naive Bayes prediction times\n","baseline_total_pred_time, baseline_time_per_pred = pred_timer(model_0, val_sentences)\n","baseline_total_pred_time, baseline_time_per_pred"],"execution_count":150,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.017347836999761057, 2.2766190288400338e-05)"]},"metadata":{},"execution_count":150}]},{"cell_type":"markdown","metadata":{"id":"nqNnKMxhz8Kl"},"source":["> `model_6` takes over 10x the time to make predictions as the baseline model. Is the accuracy boost worth the speed/time it takes to perform each prediction?"]},{"cell_type":"code","metadata":{"id":"ANKHEfRN7Nhd","colab":{"base_uri":"https://localhost:8080/","height":458},"outputId":"efb8e594-21fb-4e03-9e28-ec8c4b78cff9","executionInfo":{"status":"ok","timestamp":1660695093660,"user_tz":300,"elapsed":487,"user":{"displayName":"Hector Alejandro Fernandez","userId":"11581287658742360806"}}},"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 7))\n","plt.scatter(baseline_time_per_pred, baseline_results[\"f1\"], label=\"baseline\")\n","plt.scatter(model_6_time_per_pred, model_6_results[\"f1\"], label=\"tf_hub_sentence_encoder\")\n","plt.legend()\n","plt.title(\"F1-score versus time per prediction\")\n","plt.xlabel(\"Time per prediction\")\n","plt.ylabel(\"F1-Score\");"],"execution_count":153,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"DJWGI6GpH4Gl"},"source":["## 🛠 Exercises\n","\n","1. Rebuild, compile and train `model_1`, `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API.\n","2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data?\n","3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.\n","\n","```\n","We can use this encoding layer in place of our text_vectorizer and embedding layer\n","\n","sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n","                                        input_shape=[],\n","                                        dtype=tf.string,\n","                                        trainable=True) # turn training on to fine-tune the TensorFlow Hub model\n","```\n","4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this, [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data), how did your model perform?\n","5. Combine the ensemble predictions using the majority vote (mode), how does this perform compare to averaging the prediction probabilities of each model?\n","6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels."]},{"cell_type":"markdown","metadata":{"id":"BarVJji8H6M4"},"source":["## 📖 Extra-curriculum \n","\n","To practice what you've learned, a good idea would be to spend an hour on 3 of the following (3-hours total, you could through them all if you want) and then write a blog post about what you've learned.\n","\n","* For an overview of the different problems within NLP and how to solve them read through: \n"," * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n"," * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n","* Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building.\n","* Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook. \n","* For more on RNN's in TensorFlow, read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide, but it's worth writing the code again for yourself.\n","* Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models, it's worth checking out the following:\n"," * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).\n","  * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python.\n","* This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens, read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).  \n","  * For an even deeper dive, you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course. \n","* Great blog posts to read:\n","  * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.\n","  * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.\n","  * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery.\n","* Other topics worth looking into:\n","  * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.\n","  * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.\n","    * And now [HuggingFace even have their own course](https://huggingface.co/course/chapter1) on how their library works! I haven't done it but anything HuggingFace makes is world-class.\n","\n"]}]}